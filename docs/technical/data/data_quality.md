# äº¤é€šè³‡æ–™å“è³ªæª¢æŸ¥æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æª”èªªæ˜äº¤é€šè³‡æ–™çš„å“è³ªæ¨™æº–ã€å¸¸è¦‹å•é¡Œã€æª¢æŸ¥æ–¹æ³•å’Œè§£æ±ºæ–¹æ¡ˆã€‚

## è³‡æ–™å“è³ªæ¨™æº–

### é æœŸå“è³ªæŒ‡æ¨™

| æŒ‡æ¨™ | æ­£å¸¸ç¯„åœ | å„ªè‰¯ç¯„åœ | è­¦å‘Šç¯„åœ | èªªæ˜ |
|------|----------|----------|----------|------|
| **VD åŒ¹é…ç‡** | 80-90% | >90% | <80% | VDList èˆ‡ VDLiveList çš„ ID åŒ¹é…æ¯”ä¾‹ |
| **æœ‰æ•ˆè³‡æ–™æ¯”ä¾‹** | 60-85% | >85% | <60% | éæ¿¾éŒ¯èª¤ç¢¼å¾Œçš„æœ‰æ•ˆæ•¸å€¼æ¯”ä¾‹ |
| **NaN æ¯”ä¾‹** | 15-40% | <15% | >40% | åŒ…å«éŒ¯èª¤ç¢¼éæ¿¾å’Œç¼ºå¤±è³‡æ–™ |
| **æ™‚é–“è¦†è“‹ç‡** | >95% | >98% | <95% | é€£çºŒæ™‚é–“æ­¥çš„è³‡æ–™å®Œæ•´æ€§ |
| **ç©ºé–“è¦†è“‹ç‡** | >80% | >90% | <80% | åœ°ç†å€åŸŸå…§ VD çš„è³‡æ–™å®Œæ•´æ€§ |

### ç‰¹å¾µç¯„åœæ¨™æº–

| ç‰¹å¾µ | æ­£å¸¸ç¯„åœ | å…¸å‹å€¼ | ç•°å¸¸é–¾å€¼ | è™•ç†æ–¹å¼ |
|------|----------|--------|----------|----------|
| **avg_speed** | 0-120 km/h | 20-80 km/h | >200 æˆ– <0 | éæ¿¾ç•°å¸¸å€¼ |
| **total_volume** | 0-100 è¼›/åˆ† | 0-20 è¼›/åˆ† | >500 æˆ– <0 | æª¢æŸ¥èšåˆé‚è¼¯ |
| **avg_occupancy** | 0-50% | 0-10% | >100 æˆ– <0 | éæ¿¾ç•°å¸¸å€¼ |
| **speed_std** | 0-50 km/h | 0-20 km/h | >100 | æª¢æŸ¥è¨ˆç®—é‚è¼¯ |
| **lane_count** | 1-6 | 2-3 | >10 æˆ– <1 | æª¢æŸ¥è³‡æ–™çµæ§‹ |

## éŒ¯èª¤ç¢¼è­˜åˆ¥

### å¸¸è¦‹éŒ¯èª¤æ¨¡å¼

| éŒ¯èª¤ç¢¼/æ¨¡å¼ | å‡ºç¾åŸå›  | è­˜åˆ¥æ–¹æ³• | è™•ç†ç­–ç•¥ |
|-------------|----------|----------|----------|
| **-99** | æ„Ÿæ¸¬å™¨æ•…éšœ | ç²¾ç¢ºåŒ¹é… | è½‰æ›ç‚º NaN |
| **-1** | åˆå§‹åŒ–éŒ¯èª¤ | ç²¾ç¢ºåŒ¹é… | è½‰æ›ç‚º NaN |
| **255** | æ•¸å€¼æº¢å‡º | ç²¾ç¢ºåŒ¹é… | è½‰æ›ç‚º NaN |
| **999** | ç³»çµ±éŒ¯èª¤ | ç²¾ç¢ºåŒ¹é… | è½‰æ›ç‚º NaN |
| **é‡è¤‡æ•¸å€¼** | ç³»çµ±å¡æ­» | çµ±è¨ˆæª¢æ¸¬ | æª¢æŸ¥æ™‚é–“åºåˆ— |
| **çªç™¼ç•°å¸¸** | é€šè¨ŠéŒ¯èª¤ | ç•°å¸¸å€¼æª¢æ¸¬ | å¹³æ»‘è™•ç† |

### éŒ¯èª¤ç¢¼æª¢æ¸¬è…³æœ¬

```python
def detect_error_codes(data, feature_name):
    """æª¢æ¸¬ä¸¦å›å ±éŒ¯èª¤ç¢¼ã€‚"""
    import numpy as np
    
    # å¸¸è¦‹éŒ¯èª¤ç¢¼
    error_codes = [-99, -1, 255, 999]
    error_stats = {}
    
    for code in error_codes:
        count = np.sum(data == code)
        if count > 0:
            error_stats[code] = {
                'count': count,
                'percentage': count / data.size * 100
            }
    
    # æª¢æŸ¥ç•°å¸¸ç¯„åœ
    if feature_name == 'speed':
        invalid_count = np.sum((data < -200) | (data > 300))
    elif feature_name == 'occupancy':
        invalid_count = np.sum((data < -200) | (data > 200))
    else:
        invalid_count = np.sum((data < -1000) | (data > 1000))
    
    if invalid_count > 0:
        error_stats['out_of_range'] = {
            'count': invalid_count,
            'percentage': invalid_count / data.size * 100
        }
    
    return error_stats
```

## è³‡æ–™å“è³ªæª¢æŸ¥å·¥å…·

### 1. H5 æª”æ¡ˆå“è³ªæª¢æŸ¥

```python
import h5py
import numpy as np

def comprehensive_quality_check(h5_file_path):
    """å…¨é¢çš„ H5 æª”æ¡ˆå“è³ªæª¢æŸ¥ã€‚"""
    
    with h5py.File(h5_file_path, 'r') as f:
        features = f['data/features']
        feature_names = [name.decode() for name in f['metadata/feature_names']]
        timestamps = [ts.decode() for ts in f['metadata/timestamps']]
        
        print(f"=== H5 æª”æ¡ˆå“è³ªå ±å‘Š: {h5_file_path} ===")
        print(f"è³‡æ–™ç¶­åº¦: {features.shape}")
        print(f"æ™‚é–“ç¯„åœ: {timestamps[0]} åˆ° {timestamps[-1]}")
        print(f"ç‰¹å¾µ: {feature_names}")
        
        # æ•´é«”çµ±è¨ˆ
        total_values = features.size
        nan_values = np.sum(np.isnan(features))
        valid_values = total_values - nan_values
        
        print(f"\n=== æ•´é«”çµ±è¨ˆ ===")
        print(f"ç¸½æ•¸å€¼: {total_values:,}")
        print(f"æœ‰æ•ˆå€¼: {valid_values:,} ({valid_values/total_values:.1%})")
        print(f"NaN å€¼: {nan_values:,} ({nan_values/total_values:.1%})")
        
        # å„ç‰¹å¾µçµ±è¨ˆ
        print(f"\n=== å„ç‰¹å¾µå“è³ª ===")
        for i, fname in enumerate(feature_names):
            feature_data = features[:, :, i]
            valid_data = feature_data[~np.isnan(feature_data)]
            
            print(f"\n{fname}:")
            print(f"  æœ‰æ•ˆå€¼: {len(valid_data):,}/{feature_data.size:,} ({len(valid_data)/feature_data.size:.1%})")
            
            if len(valid_data) > 0:
                print(f"  ç¯„åœ: {valid_data.min():.2f} - {valid_data.max():.2f}")
                print(f"  å¹³å‡: {valid_data.mean():.2f}")
                print(f"  æ¨™æº–å·®: {valid_data.std():.2f}")
                
                # ç•°å¸¸å€¼æª¢æŸ¥
                if fname == 'avg_speed':
                    anomalies = np.sum((valid_data < 0) | (valid_data > 200))
                elif fname == 'avg_occupancy':
                    anomalies = np.sum((valid_data < 0) | (valid_data > 100))
                elif fname in ['total_volume', 'lane_count']:
                    anomalies = np.sum(valid_data < 0)
                else:
                    anomalies = 0
                
                if anomalies > 0:
                    print(f"  âš ï¸ ç•°å¸¸å€¼: {anomalies} ({anomalies/len(valid_data):.1%})")
                else:
                    print(f"  âœ… ç„¡ç•°å¸¸å€¼")
        
        # VD è¦†è“‹ç‡
        print(f"\n=== VD è¦†è“‹ç‡ ===")
        locations_with_data = 0
        for vd_idx in range(features.shape[1]):
            vd_data = features[:, vd_idx, :]
            if not np.all(np.isnan(vd_data)):
                locations_with_data += 1
        
        coverage = locations_with_data / features.shape[1]
        print(f"æœ‰è³‡æ–™çš„ VD: {locations_with_data}/{features.shape[1]} ({coverage:.1%})")
        
        # æ™‚é–“å®Œæ•´æ€§
        print(f"\n=== æ™‚é–“å®Œæ•´æ€§ ===")
        time_completeness = []
        for t in range(features.shape[0]):
            timestep_data = features[t, :, :]
            valid_ratio = np.sum(~np.isnan(timestep_data)) / timestep_data.size
            time_completeness.append(valid_ratio)
        
        avg_completeness = np.mean(time_completeness)
        min_completeness = np.min(time_completeness)
        print(f"å¹³å‡å®Œæ•´æ€§: {avg_completeness:.1%}")
        print(f"æœ€ä½å®Œæ•´æ€§: {min_completeness:.1%}")
        
        # å“è³ªç­‰ç´šè©•ä¼°
        print(f"\n=== å“è³ªè©•ä¼° ===")
        quality_score = 0
        
        if coverage >= 0.9:
            quality_score += 25
            print("âœ… VD è¦†è“‹ç‡: å„ªç§€")
        elif coverage >= 0.8:
            quality_score += 15
            print("âš ï¸ VD è¦†è“‹ç‡: è‰¯å¥½")
        else:
            quality_score += 5
            print("âŒ VD è¦†è“‹ç‡: éœ€æ”¹å–„")
        
        if valid_values/total_values >= 0.85:
            quality_score += 25
            print("âœ… æœ‰æ•ˆè³‡æ–™æ¯”ä¾‹: å„ªç§€")
        elif valid_values/total_values >= 0.6:
            quality_score += 15
            print("âš ï¸ æœ‰æ•ˆè³‡æ–™æ¯”ä¾‹: è‰¯å¥½")
        else:
            quality_score += 5
            print("âŒ æœ‰æ•ˆè³‡æ–™æ¯”ä¾‹: éœ€æ”¹å–„")
        
        if avg_completeness >= 0.95:
            quality_score += 25
            print("âœ… æ™‚é–“å®Œæ•´æ€§: å„ªç§€")
        elif avg_completeness >= 0.8:
            quality_score += 15
            print("âš ï¸ æ™‚é–“å®Œæ•´æ€§: è‰¯å¥½")
        else:
            quality_score += 5
            print("âŒ æ™‚é–“å®Œæ•´æ€§: éœ€æ”¹å–„")
        
        # ç•°å¸¸å€¼æª¢æŸ¥
        total_anomalies = 0
        for i, fname in enumerate(feature_names):
            feature_data = features[:, :, i]
            valid_data = feature_data[~np.isnan(feature_data)]
            if len(valid_data) > 0:
                if fname == 'avg_speed':
                    total_anomalies += np.sum((valid_data < 0) | (valid_data > 200))
                elif fname == 'avg_occupancy':
                    total_anomalies += np.sum((valid_data < 0) | (valid_data > 100))
        
        if total_anomalies == 0:
            quality_score += 25
            print("âœ… ç„¡ç•°å¸¸å€¼")
        elif total_anomalies < valid_values * 0.01:
            quality_score += 15
            print("âš ï¸ å°‘é‡ç•°å¸¸å€¼")
        else:
            quality_score += 5
            print("âŒ ç•°å¸¸å€¼éå¤š")
        
        print(f"\n=== ç¸½é«”å“è³ªè©•åˆ†: {quality_score}/100 ===")
        if quality_score >= 80:
            print("ğŸ‰ è³‡æ–™å“è³ªå„ªç§€ï¼Œå¯ç›´æ¥ç”¨æ–¼è¨“ç·´")
        elif quality_score >= 60:
            print("âš ï¸ è³‡æ–™å“è³ªè‰¯å¥½ï¼Œå»ºè­°é€²ä¸€æ­¥æ¸…ç†")
        else:
            print("âŒ è³‡æ–™å“è³ªä¸ä½³ï¼Œéœ€è¦é‡æ–°è™•ç†")

# ä½¿ç”¨ç¯„ä¾‹
comprehensive_quality_check('blob/dataset/pre-processed/h5/traffic_features.h5')
```

### 2. åŸå§‹ JSON å“è³ªæª¢æŸ¥

```python
def check_json_quality(json_dir_path):
    """æª¢æŸ¥åŸå§‹ JSON è³‡æ–™å“è³ªã€‚"""
    from social_xlstm.dataset.utils.json_utils import VDLiveList
    import os
    
    time_dirs = sorted([d for d in os.listdir(json_dir_path) 
                       if os.path.isdir(os.path.join(json_dir_path, d))])
    
    print(f"=== JSON è³‡æ–™å“è³ªæª¢æŸ¥: {json_dir_path} ===")
    print(f"æ™‚é–“ç›®éŒ„æ•¸é‡: {len(time_dirs)}")
    
    # æŠ½æ¨£æª¢æŸ¥
    sample_dirs = time_dirs[::max(1, len(time_dirs)//10)]  # å–10%æ¨£æœ¬
    
    error_code_stats = {}
    vd_count_stats = []
    data_availability_stats = []
    
    for time_dir in sample_dirs:
        vd_live_file = os.path.join(json_dir_path, time_dir, 'VDLiveList.json')
        
        try:
            vd_live_list = VDLiveList.load_from_json(vd_live_file)
            vd_count_stats.append(len(vd_live_list.LiveTrafficData))
            
            vds_with_data = 0
            speed_values = []
            occupancy_values = []
            volume_values = []
            
            for vd in vd_live_list.LiveTrafficData:
                has_data = False
                if vd.LinkFlows:
                    for link in vd.LinkFlows:
                        if link.Lanes:
                            has_data = True
                            for lane in link.Lanes:
                                speed_values.append(lane.Speed)
                                occupancy_values.append(lane.Occupancy)
                                if lane.Vehicles:
                                    for vehicle in lane.Vehicles:
                                        volume_values.append(vehicle.Volume)
                
                if has_data:
                    vds_with_data += 1
            
            data_availability_stats.append(vds_with_data / len(vd_live_list.LiveTrafficData))
            
            # éŒ¯èª¤ç¢¼çµ±è¨ˆ
            for values, name in [(speed_values, 'speed'), 
                               (occupancy_values, 'occupancy'), 
                               (volume_values, 'volume')]:
                error_stats = detect_error_codes(np.array(values), name)
                for code, stats in error_stats.items():
                    key = f"{name}_{code}"
                    if key not in error_code_stats:
                        error_code_stats[key] = []
                    error_code_stats[key].append(stats['percentage'])
                    
        except Exception as e:
            print(f"âš ï¸ è™•ç† {time_dir} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    # å ±å‘Šçµæœ
    print(f"\n=== VD æ•¸é‡çµ±è¨ˆ ===")
    print(f"å¹³å‡ VD æ•¸é‡: {np.mean(vd_count_stats):.0f}")
    print(f"VD æ•¸é‡ç¯„åœ: {np.min(vd_count_stats)} - {np.max(vd_count_stats)}")
    
    print(f"\n=== è³‡æ–™å¯ç”¨æ€§ ===")
    print(f"å¹³å‡æœ‰è³‡æ–™çš„ VD æ¯”ä¾‹: {np.mean(data_availability_stats):.1%}")
    print(f"æœ€ä½/æœ€é«˜æ¯”ä¾‹: {np.min(data_availability_stats):.1%} / {np.max(data_availability_stats):.1%}")
    
    print(f"\n=== éŒ¯èª¤ç¢¼çµ±è¨ˆ ===")
    for key, percentages in error_code_stats.items():
        avg_percentage = np.mean(percentages)
        if avg_percentage > 0.1:  # åªé¡¯ç¤º >0.1% çš„éŒ¯èª¤
            print(f"{key}: å¹³å‡ {avg_percentage:.1%} (ç¯„åœ: {np.min(percentages):.1%}-{np.max(percentages):.1%})")
```

## å¸¸è¦‹å“è³ªå•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

### å•é¡Œ 1: VD åŒ¹é…ç‡éä½ (<80%)

**å¯èƒ½åŸå› **ï¼š
- VDList å’Œ VDLiveList ä¾†æºä¸åŒæ­¥
- æ™‚é–“ç¯„åœé¸æ“‡å•é¡Œ
- éƒ¨åˆ†å€åŸŸ VD å¤§é‡æ•…éšœ

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æª¢æŸ¥ä¸åŒ¹é…çš„ VD
missing_vds = set(vd_list_ids) - set(vd_live_ids)
extra_vds = set(vd_live_ids) - set(vd_list_ids)

# åˆ†æåœ°ç†åˆ†å¸ƒ
for vd_id in missing_vds:
    vd_info = find_vd_info(vd_id)
    print(f"ç¼ºå¤± VD: {vd_id}, ä½ç½®: {vd_info.RoadName}")
```

### å•é¡Œ 2: ç•°å¸¸å€¼éå¤š

**å¯èƒ½åŸå› **ï¼š
- éŒ¯èª¤ç¢¼éæ¿¾ä¸å®Œæ•´
- æ–°çš„éŒ¯èª¤ç¢¼é¡å‹
- æ„Ÿæ¸¬å™¨æ ¡æº–å•é¡Œ

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æ“´å±•éŒ¯èª¤ç¢¼æª¢æ¸¬
def enhanced_error_detection(data):
    # æª¢æŸ¥çµ±è¨ˆç•°å¸¸
    q99 = np.percentile(data, 99)
    q01 = np.percentile(data, 1)
    
    # å¯èƒ½çš„æ–°éŒ¯èª¤ç¢¼
    potential_errors = []
    for value in np.unique(data):
        count = np.sum(data == value)
        if count > len(data) * 0.05:  # å‡ºç¾é »ç‡ >5%
            potential_errors.append(value)
    
    return potential_errors
```

### å•é¡Œ 3: æ™‚é–“å®Œæ•´æ€§ä¸è¶³

**å¯èƒ½åŸå› **ï¼š
- ç¶²è·¯å‚³è¼¸ä¸­æ–·
- ç³»çµ±ç¶­è­·æ™‚æ®µ
- è³‡æ–™å£“ç¸®æª”éºå¤±

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# æª¢æŸ¥æ™‚é–“é–“éš”
def check_time_gaps(timestamps):
    import pandas as pd
    
    timestamps_dt = pd.to_datetime(timestamps)
    time_diffs = timestamps_dt.diff()[1:]  # è·³éç¬¬ä¸€å€‹ NaT
    
    expected_interval = pd.Timedelta(minutes=5)  # å‡è¨­5åˆ†é˜é–“éš”
    large_gaps = time_diffs[time_diffs > expected_interval * 2]
    
    print(f"ç™¼ç¾ {len(large_gaps)} å€‹æ™‚é–“é–“éš”ç•°å¸¸")
    for gap in large_gaps:
        print(f"ç•°å¸¸é–“éš”: {gap}")
```

## è‡ªå‹•åŒ–å“è³ªç›£æ§

### å»ºç«‹å“è³ªç›£æ§è…³æœ¬
```python
#!/usr/bin/env python3
"""
äº¤é€šè³‡æ–™å“è³ªç›£æ§è…³æœ¬
å®šæœŸæª¢æŸ¥è³‡æ–™å“è³ªä¸¦ç”Ÿæˆå ±å‘Š
"""

def automated_quality_monitor():
    import os
    import datetime
    
    # é…ç½®
    h5_file = 'blob/dataset/pre-processed/h5/traffic_features.h5'
    json_dir = 'blob/dataset/pre-processed/unzip_to_json'
    report_dir = 'blob/reports/quality'
    
    os.makedirs(report_dir, exist_ok=True)
    
    # ç”Ÿæˆå ±å‘Š
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    report_file = f"{report_dir}/quality_report_{timestamp}.txt"
    
    with open(report_file, 'w') as f:
        # é‡å®šå‘è¼¸å‡ºåˆ°æª”æ¡ˆ
        import sys
        original_stdout = sys.stdout
        sys.stdout = f
        
        try:
            print(f"å“è³ªæª¢æŸ¥å ±å‘Š - {datetime.datetime.now()}")
            print("=" * 50)
            
            if os.path.exists(h5_file):
                comprehensive_quality_check(h5_file)
            
            if os.path.exists(json_dir):
                check_json_quality(json_dir)
                
        finally:
            sys.stdout = original_stdout
    
    print(f"å“è³ªå ±å‘Šå·²ç”Ÿæˆ: {report_file}")

if __name__ == "__main__":
    automated_quality_monitor()
```

## å“è³ªæ”¹å–„å»ºè­°

### çŸ­æœŸæ”¹å–„ (1-2é€±)
1. **å®Œå–„éŒ¯èª¤ç¢¼éæ¿¾**ï¼šæ“´å±•å·²çŸ¥éŒ¯èª¤ç¢¼æ¸…å–®
2. **ç•°å¸¸å€¼æª¢æ¸¬**ï¼šå¯¦æ–½çµ±è¨ˆå‹ç•°å¸¸å€¼æª¢æ¸¬
3. **å³æ™‚ç›£æ§**ï¼šå»ºç«‹æ¯æ—¥å“è³ªæª¢æŸ¥å ±å‘Š

### ä¸­æœŸæ”¹å–„ (1-2å€‹æœˆ)
1. **æ™ºèƒ½å¡«è£œ**ï¼šä½¿ç”¨æ™‚é–“åºåˆ—æ’å€¼å¡«è£œç¼ºå¤±å€¼
2. **ç©ºé–“æ ¡æ­£**ï¼šåˆ©ç”¨ç›¸é„° VD é€²è¡Œè³‡æ–™æ ¡æ­£
3. **æ„Ÿæ¸¬å™¨å¥åº·ç›£æ§**ï¼šè¿½è¹¤å€‹åˆ¥ VD çš„ç©©å®šæ€§

### é•·æœŸæ”¹å–„ (3-6å€‹æœˆ)
1. **æ©Ÿå™¨å­¸ç¿’æ¸…ç†**ï¼šè¨“ç·´ç•°å¸¸æª¢æ¸¬æ¨¡å‹
2. **å¤šæºèåˆ**ï¼šæ•´åˆå…¶ä»–äº¤é€šè³‡æ–™æº
3. **é æ¸¬æ€§ç¶­è­·**ï¼šé æ¸¬æ„Ÿæ¸¬å™¨æ•…éšœ

## ç›¸é—œå·¥å…·èˆ‡è…³æœ¬

- [å“è³ªæª¢æŸ¥è…³æœ¬](../scripts/utils/quality_check.py)
- [éŒ¯èª¤ç¢¼çµ±è¨ˆå·¥å…·](../scripts/utils/error_code_analysis.py)
- [è‡ªå‹•åŒ–ç›£æ§ç³»çµ±](../scripts/monitoring/quality_monitor.py)