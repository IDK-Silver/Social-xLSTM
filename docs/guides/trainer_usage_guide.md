# çµ±ä¸€è¨“ç·´ç³»çµ±ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—èªªæ˜å¦‚ä½•ä½¿ç”¨ Social-xLSTM å°ˆæ¡ˆçš„çµ±ä¸€è¨“ç·´ç³»çµ±ï¼ˆ`src/social_xlstm/training/trainer.py`ï¼‰ã€‚

## ğŸ“‹ ç›®éŒ„

1. [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
2. [æ ¸å¿ƒçµ„ä»¶](#æ ¸å¿ƒçµ„ä»¶)
3. [è©³ç´°ä½¿ç”¨èªªæ˜](#è©³ç´°ä½¿ç”¨èªªæ˜)
4. [é…ç½®é¸é …](#é…ç½®é¸é …)
5. [å¯¦éš›ç¯„ä¾‹](#å¯¦éš›ç¯„ä¾‹)
6. [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æœ€ç°¡å–®çš„ä½¿ç”¨æ–¹å¼

```python
from social_xlstm.training.trainer import Trainer, TrainingConfig
from social_xlstm.models.lstm import TrafficLSTM
from social_xlstm.dataset import TrafficDataModule

# 1. æº–å‚™æ•¸æ“š
data_module = TrafficDataModule(
    data_path="path/to/traffic_features.h5",
    batch_size=32,
    sequence_length=12
)
data_module.setup()

# 2. å‰µå»ºæ¨¡å‹
model = TrafficLSTM.create_single_vd_model()

# 3. é…ç½®è¨“ç·´
config = TrainingConfig(
    epochs=50,
    experiment_name="my_first_experiment"
)

# 4. é–‹å§‹è¨“ç·´
trainer = Trainer(
    model=model,
    training_config=config,
    train_loader=data_module.train_dataloader(),
    val_loader=data_module.val_dataloader(),
    test_loader=data_module.test_dataloader()
)

# 5. åŸ·è¡Œè¨“ç·´
history = trainer.train()
```

## ğŸ§  æ ¸å¿ƒçµ„ä»¶

### TrainingConfig - è¨“ç·´é…ç½®é¡
æ§åˆ¶è¨“ç·´éç¨‹çš„æ‰€æœ‰åƒæ•¸ï¼š

```python
@dataclass
class TrainingConfig:
    # åŸºç¤è¨“ç·´åƒæ•¸
    epochs: int = 100
    batch_size: int = 32
    learning_rate: float = 0.001
    
    # å„ªåŒ–å™¨é¸æ“‡
    optimizer_type: str = "adam"  # adam, sgd, adamw
    
    # å­¸ç¿’ç‡èª¿åº¦
    scheduler_type: str = "reduce_on_plateau"
    
    # æ—©åœæ©Ÿåˆ¶
    early_stopping_patience: int = 20
    
    # å¯¦é©—ç®¡ç†
    experiment_name: str = "traffic_lstm_experiment"
    save_dir: str = "experiments"
```

### Trainer - æ ¸å¿ƒè¨“ç·´é¡
æä¾›å®Œæ•´çš„è¨“ç·´åŠŸèƒ½ï¼š

```python
class Trainer:
    def __init__(self, model, training_config, train_loader, val_loader, test_loader)
    def train()  # ä¸»è¨“ç·´å¾ªç’°
    def evaluate_test_set()  # æ¸¬è©¦é›†è©•ä¼°
    def save_checkpoint()  # ä¿å­˜æª¢æŸ¥é»
    def plot_training_curves()  # ç¹ªè£½è¨“ç·´æ›²ç·š
```

## ğŸ“– è©³ç´°ä½¿ç”¨èªªæ˜

### 1. æ•¸æ“šæº–å‚™

```python
from social_xlstm.dataset import TrafficDataModule

# æ–¹å¼1: ä½¿ç”¨ TrafficDataModule
data_module = TrafficDataModule(
    data_path="blob/dataset/pre-processed/h5/traffic_features.h5",
    batch_size=32,
    sequence_length=12,
    train_split=0.7,
    val_split=0.15,
    test_split=0.15
)
data_module.setup()

train_loader = data_module.train_dataloader()
val_loader = data_module.val_dataloader()
test_loader = data_module.test_dataloader()
```

### 2. æ¨¡å‹å‰µå»º

```python
from social_xlstm.models.lstm import TrafficLSTM

# æ–¹å¼1: ä½¿ç”¨ä¾¿åˆ©æ–¹æ³•
model = TrafficLSTM.create_single_vd_model(
    hidden_size=128,
    num_layers=2,
    dropout=0.2
)

# æ–¹å¼2: æ‰‹å‹•é…ç½®
from social_xlstm.models.lstm import TrafficLSTMConfig

config = TrafficLSTMConfig(
    input_size=3,
    hidden_size=64,
    num_layers=3,
    output_size=3,
    dropout=0.3
)
model = TrafficLSTM(config)
```

### 3. è¨“ç·´é…ç½®

```python
from social_xlstm.training.trainer import TrainingConfig

# åŸºæœ¬é…ç½®
config = TrainingConfig(
    epochs=100,
    batch_size=32,
    learning_rate=0.001,
    experiment_name="traffic_prediction_v1"
)

# é€²éšé…ç½®
advanced_config = TrainingConfig(
    # è¨“ç·´åƒæ•¸
    epochs=200,
    batch_size=64,
    learning_rate=0.0005,
    weight_decay=1e-5,
    
    # å„ªåŒ–å™¨é…ç½®
    optimizer_type="adamw",
    betas=(0.9, 0.999),
    
    # å­¸ç¿’ç‡èª¿åº¦
    use_scheduler=True,
    scheduler_type="cosine",
    
    # æ—©åœå’Œæª¢æŸ¥é»
    early_stopping_patience=15,
    save_checkpoints=True,
    checkpoint_interval=5,
    
    # æ€§èƒ½å„ªåŒ–
    mixed_precision=True,
    gradient_clip_value=1.0,
    
    # å¯¦é©—ç®¡ç†
    experiment_name="advanced_traffic_model",
    save_dir="experiments",
    
    # è¦–è¦ºåŒ–
    plot_training_curves=True,
    plot_predictions=True,
    plot_interval=10
)
```

### 4. åŸ·è¡Œè¨“ç·´

```python
# å‰µå»ºè¨“ç·´å™¨
trainer = Trainer(
    model=model,
    training_config=config,
    train_loader=train_loader,
    val_loader=val_loader,
    test_loader=test_loader
)

# é–‹å§‹è¨“ç·´
print("é–‹å§‹è¨“ç·´...")
history = trainer.train()

# æŸ¥çœ‹è¨“ç·´çµæœ
print("è¨“ç·´å®Œæˆï¼")
print(f"æœ€ä½³é©—è­‰æå¤±: {trainer.best_val_loss:.6f}")
print(f"è¨“ç·´æ­·å²: {history}")
```

## âš™ï¸ é…ç½®é¸é …

### å„ªåŒ–å™¨é¸æ“‡
```python
# Adam å„ªåŒ–å™¨ (é è¨­)
TrainingConfig(optimizer_type="adam", learning_rate=0.001)

# AdamW å„ªåŒ–å™¨ (é©åˆå¤§æ¨¡å‹)
TrainingConfig(optimizer_type="adamw", weight_decay=0.01)

# SGD å„ªåŒ–å™¨ (ç¶“å…¸é¸æ“‡)
TrainingConfig(optimizer_type="sgd", momentum=0.9)
```

### å­¸ç¿’ç‡èª¿åº¦
```python
# å¹³å°é™ä½ (é è¨­)
TrainingConfig(
    scheduler_type="reduce_on_plateau",
    scheduler_patience=10,
    scheduler_factor=0.5
)

# éšæ¢¯é™ä½
TrainingConfig(
    scheduler_type="step",
    scheduler_step_size=30,
    scheduler_factor=0.1
)

# é¤˜å¼¦é€€ç«
TrainingConfig(scheduler_type="cosine")
```

### æ€§èƒ½å„ªåŒ–
```python
# GPU åŠ é€Ÿé…ç½®
TrainingConfig(
    device="cuda",
    mixed_precision=True,  # æ··åˆç²¾åº¦è¨“ç·´
    gradient_clip_value=1.0,  # æ¢¯åº¦è£å‰ª
    num_workers=4  # æ•¸æ“šè¼‰å…¥ä¸¦è¡Œ
)
```

## ğŸ’» å¯¦éš›ç¯„ä¾‹

### ç¯„ä¾‹1: åŸºæœ¬å–®VDæ¨¡å‹è¨“ç·´

```python
import torch
from social_xlstm.training.trainer import Trainer, TrainingConfig
from social_xlstm.models.lstm import TrafficLSTM
from social_xlstm.dataset import TrafficDataModule

def train_single_vd_model():
    """è¨“ç·´å–®VD LSTMæ¨¡å‹"""
    
    # 1. æ•¸æ“šæº–å‚™
    print("æº–å‚™æ•¸æ“š...")
    data_module = TrafficDataModule(
        data_path="blob/dataset/pre-processed/h5/traffic_features.h5",
        batch_size=32,
        sequence_length=12
    )
    data_module.setup()
    
    # 2. æ¨¡å‹å‰µå»º
    print("å‰µå»ºæ¨¡å‹...")
    model = TrafficLSTM.create_single_vd_model(
        hidden_size=128,
        num_layers=2,
        dropout=0.2
    )
    
    print(f"æ¨¡å‹åƒæ•¸: {model.get_model_info()}")
    
    # 3. è¨“ç·´é…ç½®
    config = TrainingConfig(
        epochs=50,
        batch_size=32,
        learning_rate=0.001,
        early_stopping_patience=10,
        experiment_name="single_vd_baseline",
        plot_training_curves=True
    )
    
    # 4. å‰µå»ºè¨“ç·´å™¨
    trainer = Trainer(
        model=model,
        training_config=config,
        train_loader=data_module.train_dataloader(),
        val_loader=data_module.val_dataloader(),
        test_loader=data_module.test_dataloader()
    )
    
    # 5. é–‹å§‹è¨“ç·´
    print("é–‹å§‹è¨“ç·´...")
    history = trainer.train()
    
    # 6. çµæœåˆ†æ
    print("\nè¨“ç·´å®Œæˆï¼")
    print(f"æœ€çµ‚è¨“ç·´æå¤±: {history['train_loss'][-1]:.6f}")
    print(f"æœ€çµ‚é©—è­‰æå¤±: {history['val_loss'][-1]:.6f}")
    
    return trainer, history

if __name__ == "__main__":
    trainer, history = train_single_vd_model()
```

### ç¯„ä¾‹2: å¤šVDæ¨¡å‹è¨“ç·´

```python
def train_multi_vd_model():
    """è¨“ç·´å¤šVD LSTMæ¨¡å‹"""
    
    # æ•¸æ“šæº–å‚™ (å¤šVDæ•¸æ“š)
    data_module = TrafficDataModule(
        data_path="blob/dataset/pre-processed/h5/traffic_features.h5",
        batch_size=16,  # è¼ƒå°æ‰¹æ¬¡ï¼Œå› ç‚ºå¤šVDæ•¸æ“šè¼ƒå¤§
        sequence_length=12,
        selected_vd_ids=['VD001', 'VD002', 'VD003', 'VD004', 'VD005']
    )
    data_module.setup()
    
    # å‰µå»ºå¤šVDæ¨¡å‹
    model = TrafficLSTM.create_multi_vd_model(
        num_vds=5,
        hidden_size=256,  # æ›´å¤§çš„éš±è—å±¤
        num_layers=3,     # æ›´å¤šå±¤æ•¸
        dropout=0.3
    )
    
    # é€²éšè¨“ç·´é…ç½®
    config = TrainingConfig(
        epochs=100,
        batch_size=16,
        learning_rate=0.0008,
        optimizer_type="adamw",
        weight_decay=0.01,
        
        # å­¸ç¿’ç‡èª¿åº¦
        use_scheduler=True,
        scheduler_type="cosine",
        
        # æ—©åœ
        early_stopping_patience=15,
        
        # æ€§èƒ½å„ªåŒ–
        mixed_precision=True,
        gradient_clip_value=1.0,
        
        # å¯¦é©—ç®¡ç†
        experiment_name="multi_vd_advanced",
        save_dir="experiments",
        
        # è¦–è¦ºåŒ–
        plot_training_curves=True,
        plot_predictions=True,
        plot_interval=10
    )
    
    # è¨“ç·´
    trainer = Trainer(model, config, 
                     data_module.train_dataloader(),
                     data_module.val_dataloader(),
                     data_module.test_dataloader())
    
    history = trainer.train()
    
    return trainer, history
```

### ç¯„ä¾‹3: å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´

```python
def resume_training():
    """å¾æª¢æŸ¥é»æ¢å¾©è¨“ç·´"""
    
    # æº–å‚™æ•¸æ“šè¼‰å…¥å™¨
    data_module = TrafficDataModule(
        data_path="blob/dataset/pre-processed/h5/traffic_features.h5",
        batch_size=32
    )
    data_module.setup()
    
    # å¾æª¢æŸ¥é»è¼‰å…¥
    trainer = Trainer.load_checkpoint(
        checkpoint_path="experiments/my_experiment/best_model.pt",
        train_loader=data_module.train_dataloader(),
        val_loader=data_module.val_dataloader(),
        test_loader=data_module.test_dataloader()
    )
    
    print(f"å¾ epoch {trainer.epoch} æ¢å¾©è¨“ç·´")
    print(f"æœ€ä½³é©—è­‰æå¤±: {trainer.best_val_loss:.6f}")
    
    # ç¹¼çºŒè¨“ç·´
    history = trainer.train()
    
    return trainer, history
```

### ç¯„ä¾‹4: è¶…åƒæ•¸èª¿å„ª

```python
def hyperparameter_tuning():
    """è¶…åƒæ•¸èª¿å„ªç¯„ä¾‹"""
    
    # è¶…åƒæ•¸çµ„åˆ
    hyperparams = [
        {'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2, 'lr': 0.001},
        {'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3, 'lr': 0.0008},
        {'hidden_size': 256, 'num_layers': 3, 'dropout': 0.4, 'lr': 0.0005},
    ]
    
    best_val_loss = float('inf')
    best_config = None
    results = []
    
    for i, params in enumerate(hyperparams):
        print(f"\nå¯¦é©— {i+1}/{len(hyperparams)}: {params}")
        
        # æ•¸æ“šæº–å‚™
        data_module = TrafficDataModule(
            data_path="blob/dataset/pre-processed/h5/traffic_features.h5",
            batch_size=32
        )
        data_module.setup()
        
        # å‰µå»ºæ¨¡å‹
        model = TrafficLSTM.create_single_vd_model(
            hidden_size=params['hidden_size'],
            num_layers=params['num_layers'],
            dropout=params['dropout']
        )
        
        # é…ç½®è¨“ç·´
        config = TrainingConfig(
            epochs=30,  # è¼ƒå°‘epochç”¨æ–¼å¿«é€Ÿè©•ä¼°
            learning_rate=params['lr'],
            early_stopping_patience=5,
            experiment_name=f"hyperparam_exp_{i+1}",
            plot_training_curves=False  # é—œé–‰è¦–è¦ºåŒ–ä»¥åŠ å¿«é€Ÿåº¦
        )
        
        # è¨“ç·´
        trainer = Trainer(model, config,
                         data_module.train_dataloader(),
                         data_module.val_dataloader(),
                         data_module.test_dataloader())
        
        history = trainer.train()
        
        # è¨˜éŒ„çµæœ
        final_val_loss = min(history['val_loss'])
        results.append({
            'params': params,
            'val_loss': final_val_loss,
            'experiment': f"hyperparam_exp_{i+1}"
        })
        
        if final_val_loss < best_val_loss:
            best_val_loss = final_val_loss
            best_config = params
        
        print(f"é©—è­‰æå¤±: {final_val_loss:.6f}")
    
    # è¼¸å‡ºæœ€ä½³çµæœ
    print(f"\næœ€ä½³é…ç½®: {best_config}")
    print(f"æœ€ä½³é©—è­‰æå¤±: {best_val_loss:.6f}")
    
    return results, best_config
```

## â“ å¸¸è¦‹å•é¡Œ

### Q1: è¨˜æ†¶é«”ä¸è¶³æ€éº¼è¾¦ï¼Ÿ

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# æ¸›å°‘æ‰¹æ¬¡å¤§å°
TrainingConfig(batch_size=16)  # æˆ–æ›´å°

# å•Ÿç”¨æ··åˆç²¾åº¦
TrainingConfig(mixed_precision=True)

# æ¸›å°‘æ¨¡å‹å¤§å°
model = TrafficLSTM.create_single_vd_model(
    hidden_size=64,  # æ¸›å°‘éš±è—å±¤å¤§å°
    num_layers=2     # æ¸›å°‘å±¤æ•¸
)
```

### Q2: è¨“ç·´é€Ÿåº¦å¤ªæ…¢æ€éº¼è¾¦ï¼Ÿ

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# å¢åŠ æ‰¹æ¬¡å¤§å°
TrainingConfig(batch_size=64)

# å•Ÿç”¨æ··åˆç²¾åº¦
TrainingConfig(mixed_precision=True)

# å¢åŠ æ•¸æ“šè¼‰å…¥å·¥ä½œé€²ç¨‹
TrainingConfig(num_workers=8)

# ä½¿ç”¨æ›´å¿«çš„å„ªåŒ–å™¨
TrainingConfig(optimizer_type="adamw")
```

### Q3: æ¨¡å‹ä¸æ”¶æ–‚æ€éº¼è¾¦ï¼Ÿ

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# é™ä½å­¸ç¿’ç‡
TrainingConfig(learning_rate=0.0001)

# å•Ÿç”¨æ¢¯åº¦è£å‰ª
TrainingConfig(gradient_clip_value=1.0)

# ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦
TrainingConfig(
    use_scheduler=True,
    scheduler_type="reduce_on_plateau"
)

# å¢åŠ æ­£å‰‡åŒ–
TrainingConfig(weight_decay=0.01)
```

### Q4: å¦‚ä½•ç›£æ§è¨“ç·´éç¨‹ï¼Ÿ

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# å•Ÿç”¨è©³ç´°æ—¥èªŒ
import logging
logging.basicConfig(level=logging.INFO)

# å•Ÿç”¨è¦–è¦ºåŒ–
TrainingConfig(
    plot_training_curves=True,
    plot_interval=10,
    log_interval=5
)

# æŸ¥çœ‹å¯¦é©—ç›®éŒ„
# experiments/your_experiment_name/
# â”œâ”€â”€ config.json          # é…ç½®æ–‡ä»¶
# â”œâ”€â”€ training_curves.png  # è¨“ç·´æ›²ç·š
# â”œâ”€â”€ predictions.png      # é æ¸¬çµæœ
# â”œâ”€â”€ test_evaluation.json # æ¸¬è©¦è©•ä¼°
# â””â”€â”€ best_model.pt        # æœ€ä½³æ¨¡å‹
```

### Q5: å¦‚ä½•æ¯”è¼ƒä¸åŒæ¨¡å‹ï¼Ÿ

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# ä½¿ç”¨ä¸åŒçš„å¯¦é©—åç¨±
configs = [
    TrainingConfig(experiment_name="lstm_small", hidden_size=64),
    TrainingConfig(experiment_name="lstm_large", hidden_size=256),
    TrainingConfig(experiment_name="lstm_deep", num_layers=4)
]

# è¨“ç·´å¤šå€‹æ¨¡å‹ä¸¦æ¯”è¼ƒçµæœ
results = []
for config in configs:
    trainer = Trainer(model, config, train_loader, val_loader, test_loader)
    history = trainer.train()
    results.append({
        'name': config.experiment_name,
        'best_val_loss': min(history['val_loss'])
    })

# æ¯”è¼ƒçµæœ
for result in sorted(results, key=lambda x: x['best_val_loss']):
    print(f"{result['name']}: {result['best_val_loss']:.6f}")
```

## ğŸ”— ç›¸é—œæ–‡æª”

- [LSTM ä½¿ç”¨æŒ‡å—](lstm_usage_guide.md)
- [æ¨¡çµ„åŠŸèƒ½èªªæ˜](../implementation/modules.md)
- [Social xLSTM æ¶æ§‹è¨­è¨ˆ](../architecture/social_xlstm_design.md)
- [å°ˆæ¡ˆæ¦‚è¿°](../overview/project_overview.md)

## ğŸ“ æ”¯æ´

å¦‚æœ‰å•é¡Œï¼Œè«‹åƒè€ƒï¼š
1. ğŸ“– é€™ä»½ä½¿ç”¨æŒ‡å—
2. ğŸ’» ç¯„ä¾‹ä»£ç¢¼ (`examples/`)
3. ğŸ› GitHub Issues
4. ğŸ“§ å°ˆæ¡ˆåœ˜éšŠè¯ç¹«æ–¹å¼

---

**æé†’**: è¨“ç·´ç³»çµ±æœƒè‡ªå‹•å‰µå»ºå¯¦é©—ç›®éŒ„ä¸¦ä¿å­˜æ‰€æœ‰çµæœï¼Œè«‹ç¢ºä¿æœ‰è¶³å¤ çš„ç£ç¢Ÿç©ºé–“ã€‚