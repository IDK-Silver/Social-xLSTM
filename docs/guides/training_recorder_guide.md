# Training Recorder ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

`TrainingRecorder` æ˜¯ Social-xLSTM å°ˆæ¡ˆçš„æ ¸å¿ƒè¨“ç·´è¨˜éŒ„ç³»çµ±ï¼Œå°ˆæ³¨æ–¼å®Œæ•´è¨˜éŒ„è¨“ç·´éç¨‹ä¸­çš„æ‰€æœ‰è³‡è¨Šã€‚å®ƒèƒ½å¤ å°‡è¨“ç·´è³‡æ–™ä¿å­˜ç‚ºå¤šç¨®æ ¼å¼ï¼Œæ”¯æ´äº‹å¾Œåˆ†æå’Œæ–°æŒ‡æ¨™è¨ˆç®—ã€‚è¦–è¦ºåŒ–åŠŸèƒ½ç”±ç¨ç«‹çš„ `TrainingVisualizer` é¡åˆ¥æä¾›ã€‚

## ğŸ¯ ä¸»è¦åŠŸèƒ½

### 1. å®Œæ•´çš„è¨“ç·´è¨˜éŒ„
- **æ¯å€‹ epoch çš„è©³ç´°è³‡è¨Š**ï¼šæå¤±ã€æŒ‡æ¨™ã€å­¸ç¿’ç‡ã€æ™‚é–“ã€è¨˜æ†¶é«”ä½¿ç”¨ç­‰
- **ç³»çµ±å…ƒæ•¸æ“š**ï¼šGit commitã€Python ç‰ˆæœ¬ã€PyTorch ç‰ˆæœ¬ã€CUDA è³‡è¨Š
- **å¯¦é©—é…ç½®**ï¼šæ¨¡å‹é…ç½®ã€è¨“ç·´é…ç½®çš„å®Œæ•´è¨˜éŒ„

### 2. æ™ºèƒ½åˆ†æ
- **æœ€ä½³ epoch è‡ªå‹•è¿½è¹¤**
- **è¨“ç·´ç©©å®šæ€§åˆ†æ**
- **éæ“¬åˆæª¢æ¸¬**
- **æ”¶æ–‚ç‹€æ…‹è©•ä¼°**

### 3. å¤šæ ¼å¼è¼¸å‡ºæ”¯æ´
- **JSON æ ¼å¼**ï¼šå®Œæ•´çš„çµæ§‹åŒ–è³‡æ–™
- **CSV æ ¼å¼**ï¼šæ–¹ä¾¿ç”¨ Excel æˆ–å…¶ä»–å·¥å…·åˆ†æ
- **TensorBoard**ï¼šæ”¯æ´å³æ™‚ç›£æ§å’Œè¦–è¦ºåŒ–
- **PKL æ ¼å¼**ï¼šåŸå§‹ Python ç‰©ä»¶ï¼Œæ–¹ä¾¿ç¨‹å¼è™•ç†

### 4. å®Œæ•´çš„è³‡æ–™ä¿å­˜
- **æ¯å€‹ epoch çš„å®Œæ•´è¨˜éŒ„**ï¼šæå¤±ã€æŒ‡æ¨™ã€ç³»çµ±ç‹€æ…‹
- **åŸå§‹è³‡æ–™ä¿å­˜**ï¼šæ”¯æ´äº‹å¾Œè¨ˆç®—æ–°æŒ‡æ¨™
- **ç³»çµ±å…ƒæ•¸æ“š**ï¼šç¢ºä¿å¯¦é©—å¯é‡ç¾
- **éˆæ´»çš„è¼‰å…¥æ©Ÿåˆ¶**ï¼šæ”¯æ´çºŒè¨“ç·´æˆ–åˆ†æ

## ğŸš€ å¿«é€Ÿé–‹å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from social_xlstm.training.recorder import TrainingRecorder

# 1. åˆå§‹åŒ–è¨˜éŒ„å™¨
recorder = TrainingRecorder(
    experiment_name="baseline_lstm_v1",
    model_config=model.config.__dict__,
    training_config=training_config.__dict__
)

# 2. åœ¨è¨“ç·´å¾ªç’°ä¸­è¨˜éŒ„
for epoch in range(epochs):
    # è¨“ç·´é‚è¼¯...
    start_time = time.time()
    
    train_metrics = train_one_epoch()
    val_metrics = validate_one_epoch()
    
    epoch_time = time.time() - start_time
    
    # è¨˜éŒ„é€™å€‹ epoch
    recorder.log_epoch(
        epoch=epoch,
        train_loss=train_metrics['loss'],
        val_loss=val_metrics['loss'],
        train_metrics=train_metrics,
        val_metrics=val_metrics,
        learning_rate=optimizer.param_groups[0]['lr'],
        epoch_time=epoch_time
    )

# 3. ä¿å­˜è¨˜éŒ„ï¼ˆå¤šç¨®æ ¼å¼ï¼‰
recorder.save("experiments/baseline_lstm_v1/training_record.json")  # JSONæ ¼å¼
recorder.export_to_csv("experiments/baseline_lstm_v1/training_history.csv")  # CSVæ ¼å¼
recorder.export_to_tensorboard("experiments/baseline_lstm_v1/tensorboard")  # TensorBoard

# 4. ä½¿ç”¨ç¨ç«‹çš„è¦–è¦ºåŒ–å·¥å…·
from social_xlstm.visualization.training_visualizer import TrainingVisualizer

visualizer = TrainingVisualizer()
visualizer.plot_training_dashboard(recorder, "experiments/baseline_lstm_v1/dashboard.png")
```

### èˆ‡ç¾æœ‰ Trainer æ•´åˆ

```python
# ä¿®æ”¹ trainer.py ä¸­çš„ __init__ æ–¹æ³•
class Trainer:
    def __init__(self, model, training_config, ...):
        # åŸæœ‰åˆå§‹åŒ–...
        
        # æ›¿æ›åŸæœ‰çš„ training_history
        self.recorder = TrainingRecorder(
            experiment_name=training_config.experiment_name,
            model_config=model.config.__dict__,
            training_config=training_config.__dict__
        )
    
    def train(self):
        for epoch in range(self.config.epochs):
            start_time = time.time()
            
            # è¨“ç·´å’Œé©—è­‰
            train_metrics = self.train_epoch()
            val_metrics = self.validate_epoch()
            
            epoch_time = time.time() - start_time
            
            # è¨˜éŒ„åˆ° recorder
            self.recorder.log_epoch(
                epoch=epoch,
                train_loss=train_metrics['train_loss'],
                val_loss=val_metrics.get('val_loss'),
                train_metrics=train_metrics,
                val_metrics=val_metrics,
                learning_rate=self.optimizer.param_groups[0]['lr'],
                epoch_time=epoch_time,
                gradient_norm=self._calculate_gradient_norm()
            )
            
            # å…¶ä»–è¨“ç·´é‚è¼¯...
        
        # ä¿å­˜å®Œæ•´è¨˜éŒ„
        self.recorder.save(self.experiment_dir / "training_record.json")
        
        return self.recorder
```

## ğŸ“Š è©³ç´°åŠŸèƒ½èªªæ˜

### 1. EpochRecord è³‡æ–™çµæ§‹

æ¯å€‹ epoch çš„è¨˜éŒ„åŒ…å«ï¼š

```python
@dataclass
class EpochRecord:
    epoch: int                              # Epoch ç·¨è™Ÿ
    timestamp: datetime                     # æ™‚é–“æˆ³
    train_loss: float                       # è¨“ç·´æå¤±
    val_loss: Optional[float]               # é©—è­‰æå¤±
    train_metrics: Dict[str, float]         # è¨“ç·´æŒ‡æ¨™ (MAE, MSE, etc.)
    val_metrics: Dict[str, float]           # é©—è­‰æŒ‡æ¨™
    learning_rate: float                    # å­¸ç¿’ç‡
    epoch_time: float                       # è¨“ç·´æ™‚é–“
    memory_usage: Optional[float]           # è¨˜æ†¶é«”ä½¿ç”¨
    gradient_norm: Optional[float]          # æ¢¯åº¦ç¯„æ•¸
    is_best: bool                          # æ˜¯å¦æœ€ä½³ epoch
    sample_predictions: Optional[Dict]      # æ¨£æœ¬é æ¸¬ï¼ˆå¯é¸ï¼‰
```

### 2. è¨˜éŒ„æ–¹æ³•

#### log_epoch() - è¨˜éŒ„å–®å€‹ epoch

```python
recorder.log_epoch(
    epoch=0,
    train_loss=0.1234,
    val_loss=0.1456,
    train_metrics={
        'mae': 0.0876,
        'mse': 0.0123,
        'rmse': 0.1110,
        'r2': 0.8765
    },
    val_metrics={
        'mae': 0.0923,
        'mse': 0.0134,
        'rmse': 0.1158,
        'r2': 0.8654
    },
    learning_rate=0.001,
    epoch_time=45.6,
    gradient_norm=0.234
)
```

### 3. æŸ¥è©¢æ–¹æ³•

#### get_metric_history() - ç²å–æŒ‡æ¨™æ­·å²

```python
# ç²å–è¨“ç·´ MAE æ­·å²
train_mae_history = recorder.get_metric_history('mae', 'train')

# ç²å–é©—è­‰ MSE æ­·å²
val_mse_history = recorder.get_metric_history('mse', 'val')
```

#### get_loss_history() - ç²å–æå¤±æ­·å²

```python
train_losses, val_losses = recorder.get_loss_history()
```

#### get_best_epoch() - ç²å–æœ€ä½³ epoch

```python
best_epoch = recorder.get_best_epoch()
print(f"æœ€ä½³ epoch: {best_epoch.epoch}")
print(f"æœ€ä½³é©—è­‰æå¤±: {best_epoch.val_loss}")
```

#### get_training_summary() - ç²å–è¨“ç·´ç¸½çµ

```python
summary = recorder.get_training_summary()
print(f"ç¸½ epochs: {summary['total_epochs']}")
print(f"ç¸½æ™‚é–“: {summary['total_time']:.2f} ç§’")
print(f"å¹³å‡ epoch æ™‚é–“: {summary['avg_epoch_time']:.2f} ç§’")
print(f"æœ€ä½³ epoch: {summary['best_epoch']}")
print(f"æœ€ä½³é©—è­‰æå¤±: {summary['best_val_loss']:.4f}")
```

### 4. åˆ†ææ–¹æ³•

#### analyze_training_stability() - è¨“ç·´ç©©å®šæ€§åˆ†æ

```python
stability = recorder.analyze_training_stability()
print(f"è¨“ç·´æå¤±è¶¨å‹¢: {stability['train_trend']:.6f}")
print(f"é©—è­‰æå¤±è¶¨å‹¢: {stability['val_trend']:.6f}")
print(f"éæ“¬åˆåˆ†æ•¸: {stability['overfitting_score']:.4f}")
print(f"æ”¶æ–‚ç‹€æ…‹: {stability['convergence_status']}")
```

### 5. è³‡æ–™è¼¸å‡ºæ–¹æ³•

#### save() - ä¿å­˜ç‚º JSON æ ¼å¼

```python
# ä¿å­˜å®Œæ•´è¨˜éŒ„
recorder.save("training_record.json")
```

JSON æª”æ¡ˆåŒ…å«ï¼š
- å¯¦é©—é…ç½®ï¼ˆæ¨¡å‹ã€è¨“ç·´åƒæ•¸ï¼‰
- æ¯å€‹ epoch çš„è©³ç´°è¨˜éŒ„
- ç³»çµ±å…ƒæ•¸æ“šï¼ˆGit commitã€ç‰ˆæœ¬è³‡è¨Šï¼‰
- è¨“ç·´ç¸½çµå’Œåˆ†æçµæœ

#### export_to_csv() - è¼¸å‡ºç‚º CSV æ ¼å¼

```python
# åŒ¯å‡ºç‚º CSVï¼Œæ–¹ä¾¿ç”¨ Excel åˆ†æ
recorder.export_to_csv("training_history.csv")
```

CSV æª”æ¡ˆåŒ…å«ï¼š
- æ¯å€‹ epoch ä¸€è¡Œ
- æ‰€æœ‰æå¤±ã€æŒ‡æ¨™ã€ç³»çµ±ç‹€æ…‹
- æ–¹ä¾¿åŒ¯å…¥å…¶ä»–åˆ†æå·¥å…·

#### export_to_tensorboard() - TensorBoard æ ¼å¼

```python
# åŒ¯å‡º TensorBoard æ—¥èªŒ
recorder.export_to_tensorboard("runs/experiment_1")

# å•Ÿå‹• TensorBoard æŸ¥çœ‹
# tensorboard --logdir=runs
```

### 6. æŒä¹…åŒ–æ–¹æ³•

#### save() - ä¿å­˜è¨˜éŒ„

```python
# ä¿å­˜ç‚º JSON æ–‡ä»¶
recorder.save("experiments/my_experiment/training_record.json")
```

ä¿å­˜çš„ JSON åŒ…å«ï¼š
- å¯¦é©—é…ç½®
- æ‰€æœ‰ epoch è¨˜éŒ„
- ç³»çµ±å…ƒæ•¸æ“š
- è¨“ç·´ç¸½çµ
- åˆ†æçµæœ

#### load() - è¼‰å…¥è¨˜éŒ„

```python
# è¼‰å…¥ä¹‹å‰ä¿å­˜çš„è¨˜éŒ„
recorder = TrainingRecorder.load("experiments/my_experiment/training_record.json")

# ç¹¼çºŒåˆ†æ
summary = recorder.get_training_summary()
recorder.plot_training_curves()
```

### 7. æ¯”è¼ƒæ–¹æ³•

#### compare_with() - å¯¦é©—æ¯”è¼ƒ

```python
# è¼‰å…¥å…©å€‹å¯¦é©—è¨˜éŒ„
recorder1 = TrainingRecorder.load("experiments/baseline/training_record.json")
recorder2 = TrainingRecorder.load("experiments/improved/training_record.json")

# æ¯”è¼ƒå¯¦é©—
comparison = recorder1.compare_with(recorder2)
print(f"è¼ƒä½³æ¨¡å‹: {comparison['better_performer']}")
print(f"æå¤±å·®ç•°: {comparison['loss_difference']:.4f}")
print(f"æ”¶æ–‚ epoch å·®ç•°: {comparison['epoch_difference']}")
```

## ğŸ“Š è¦–è¦ºåŒ–åŠŸèƒ½ï¼ˆä½¿ç”¨ TrainingVisualizerï¼‰

`TrainingVisualizer` æ˜¯ç¨ç«‹çš„è¦–è¦ºåŒ–é¡åˆ¥ï¼Œæä¾›è±å¯Œçš„åœ–è¡¨åŠŸèƒ½ï¼š

### åŸºæœ¬ä½¿ç”¨

```python
from social_xlstm.visualization.training_visualizer import TrainingVisualizer

# è¼‰å…¥è¨˜éŒ„
recorder = TrainingRecorder.load("training_record.json")

# å‰µå»ºè¦–è¦ºåŒ–å™¨
visualizer = TrainingVisualizer()

# ç”Ÿæˆå„ç¨®åœ–è¡¨
visualizer.plot_basic_training_curves(recorder, "basic_curves.png")
visualizer.plot_training_dashboard(recorder, "dashboard.png")
visualizer.plot_advanced_metrics(recorder, "advanced_metrics.png")
```

### å¯ç”¨çš„è¦–è¦ºåŒ–åŠŸèƒ½

1. **åŸºæœ¬è¨“ç·´æ›²ç·š** (`plot_basic_training_curves`)
   - æå¤±æ›²ç·šã€ä¸»è¦æŒ‡æ¨™ã€å­¸ç¿’ç‡ã€è¨“ç·´æ™‚é–“

2. **è¨“ç·´å„€è¡¨æ¿** (`plot_training_dashboard`)
   - ç¶œåˆè¦–åœ–ï¼ŒåŒ…å«å¤šå€‹é—œéµåœ–è¡¨å’Œæ‘˜è¦

3. **é€²éšæŒ‡æ¨™** (`plot_advanced_metrics`)
   - æ¢¯åº¦ç¯„æ•¸ã€è¨˜æ†¶é«”ä½¿ç”¨ã€æ”¶æ–‚åˆ†æã€éæ“¬åˆæª¢æ¸¬

4. **å¯¦é©—æ¯”è¼ƒ** (`plot_experiment_comparison`)
   - å¤šå€‹å¯¦é©—çš„ä¸¦æ’æ¯”è¼ƒ

5. **æŒ‡æ¨™æ¼”åŒ–** (`plot_metric_evolution`)
   - ç‰¹å®šæŒ‡æ¨™çš„è©³ç´°è®ŠåŒ–éç¨‹

6. **å®Œæ•´å ±å‘Š** (`create_training_report`)
   - è‡ªå‹•ç”ŸæˆåŒ…å«æ‰€æœ‰åœ–è¡¨çš„å®Œæ•´å ±å‘Š

## ğŸ”§ å¯¦éš›ä½¿ç”¨å ´æ™¯

### å ´æ™¯ 1: Baseline å¯¦é©—è¨˜éŒ„

```python
# å–® VD baseline å¯¦é©—
recorder = TrainingRecorder(
    experiment_name="single_vd_baseline",
    model_config=model.config.__dict__,
    training_config=training_config.__dict__
)

# è¨“ç·´ä¸¦è¨˜éŒ„
for epoch in range(100):
    # è¨“ç·´é‚è¼¯...
    recorder.log_epoch(epoch, train_loss, val_loss, train_metrics, val_metrics, lr, time)

# ä¿å­˜å¤šç¨®æ ¼å¼
recorder.save("experiments/single_vd_baseline/training_record.json")
recorder.export_to_csv("experiments/single_vd_baseline/history.csv")

# è¦–è¦ºåŒ–åˆ†æ
visualizer = TrainingVisualizer()
visualizer.create_training_report(recorder, "experiments/single_vd_baseline/report")

# ç²å– baseline çµæœ
baseline_summary = recorder.get_training_summary()
```

### å ´æ™¯ 2: å¯¦é©—æ¯”è¼ƒå’Œåˆ†æ

```python
# è¼‰å…¥å¤šå€‹å¯¦é©—è¨˜éŒ„
single_vd = TrainingRecorder.load("experiments/single_vd_baseline/training_record.json")
multi_vd = TrainingRecorder.load("experiments/multi_vd_baseline/training_record.json")

# æ¯”è¼ƒçµæœ
comparison = single_vd.compare_with(multi_vd)
print(f"å–® VD æœ€ä½³æå¤±: {single_vd.get_best_epoch().val_loss:.4f}")
print(f"å¤š VD æœ€ä½³æå¤±: {multi_vd.get_best_epoch().val_loss:.4f}")
print(f"æ›´ä½³æ–¹æ³•: {comparison['better_performer']}")

# åˆ†æç©©å®šæ€§
single_stability = single_vd.analyze_training_stability()
multi_stability = multi_vd.analyze_training_stability()
```

### å ´æ™¯ 3: é›¢ç·šåˆ†æå’Œæ–°æŒ‡æ¨™è¨ˆç®—

```python
# è¼‰å…¥å·²ä¿å­˜çš„è¨˜éŒ„
recorder = TrainingRecorder.load("experiments/baseline/training_record.json")

# è¨ˆç®—è‡ªå®šç¾©æŒ‡æ¨™
def custom_metric(predictions, targets):
    # è‡ªå®šç¾©æŒ‡æ¨™è¨ˆç®—é‚è¼¯
    return np.mean(np.abs(predictions - targets) / targets)

# ç²å–æ­·å²æ•¸æ“šé€²è¡Œåˆ†æ
train_mae_history = recorder.get_metric_history('mae', 'train')
val_mae_history = recorder.get_metric_history('mae', 'val')

# åˆ†ææ”¶æ–‚è¡Œç‚º
convergence_analysis = recorder._analyze_convergence()
```

## ğŸ“ˆ é€²éšåŠŸèƒ½

### 1. è‡ªå®šç¾©æŒ‡æ¨™è¨˜éŒ„

```python
# åœ¨è¨“ç·´å¾ªç’°ä¸­è¨˜éŒ„è‡ªå®šç¾©æŒ‡æ¨™
custom_metrics = {
    'mae': calculate_mae(predictions, targets),
    'custom_score': calculate_custom_score(predictions, targets),
    'feature_importance': calculate_feature_importance(model)
}

recorder.log_epoch(
    epoch=epoch,
    train_loss=train_loss,
    val_loss=val_loss,
    train_metrics=custom_metrics,
    val_metrics=custom_metrics,
    # å…¶ä»–åƒæ•¸...
)
```

### 2. æ¢¯åº¦ç›£æ§

```python
# è¨ˆç®—æ¢¯åº¦ç¯„æ•¸
def calculate_gradient_norm(model):
    total_norm = 0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    return total_norm ** (1. / 2)

# è¨˜éŒ„æ¢¯åº¦è³‡è¨Š
recorder.log_epoch(
    epoch=epoch,
    train_loss=train_loss,
    val_loss=val_loss,
    gradient_norm=calculate_gradient_norm(model),
    # å…¶ä»–åƒæ•¸...
)
```

### 3. æ¨£æœ¬é æ¸¬è¿½è¹¤

```python
# è¨˜éŒ„ä»£è¡¨æ€§æ¨£æœ¬çš„é æ¸¬
sample_predictions = {
    'sample_inputs': sample_inputs.cpu().numpy(),
    'sample_targets': sample_targets.cpu().numpy(),
    'sample_predictions': sample_outputs.cpu().numpy()
}

recorder.log_epoch(
    epoch=epoch,
    train_loss=train_loss,
    val_loss=val_loss,
    sample_predictions=sample_predictions,
    # å…¶ä»–åƒæ•¸...
)
```

## ğŸ› ï¸ æ•´åˆåˆ°ç¾æœ‰ç³»çµ±

### 1. ä¿®æ”¹ Trainer é¡

```python
# åœ¨ src/social_xlstm/training/trainer.py ä¸­ï¼š

from .recorder import TrainingRecorder

class Trainer:
    def __init__(self, model, training_config, ...):
        # ç¾æœ‰åˆå§‹åŒ–...
        
        # æ›¿æ› training_history
        self.recorder = TrainingRecorder(
            experiment_name=training_config.experiment_name,
            model_config=model.config.__dict__,
            training_config=training_config.__dict__
        )
    
    def train(self):
        for epoch in range(self.config.epochs):
            # è¨“ç·´é‚è¼¯...
            
            # ä½¿ç”¨ recorder è¨˜éŒ„
            self.recorder.log_epoch(
                epoch=epoch,
                train_loss=train_metrics['train_loss'],
                val_loss=val_metrics.get('val_loss'),
                train_metrics=train_metrics,
                val_metrics=val_metrics,
                learning_rate=self.optimizer.param_groups[0]['lr'],
                epoch_time=epoch_time
            )
        
        # ä¿å­˜è¨˜éŒ„
        self.recorder.save(self.experiment_dir / "training_record.json")
        
        return self.recorder
```

### 2. è¨“ç·´è…³æœ¬æ›´æ–°

```python
# åœ¨ scripts/train/train_single_vd.py ä¸­ï¼š

def main():
    # ç¾æœ‰é‚è¼¯...
    
    # è¨“ç·´
    trainer = Trainer(model, training_config, ...)
    recorder = trainer.train()
    
    # ç”Ÿæˆåˆ†æå ±å‘Š
    recorder.plot_training_curves(trainer.experiment_dir / "training_curves.png")
    
    # ç²å–è¨“ç·´ç¸½çµ
    summary = recorder.get_training_summary()
    logger.info(f"è¨“ç·´å®Œæˆï¼æœ€ä½³é©—è­‰æå¤±: {summary['best_val_loss']:.4f}")
```

## ğŸ“‹ æœ€ä½³å¯¦è¸

### 1. å‘½åè¦ç¯„
- ä½¿ç”¨æè¿°æ€§çš„å¯¦é©—åç¨±ï¼š`single_vd_baseline_v1`
- åŒ…å«é—œéµåƒæ•¸ï¼š`multi_vd_h256_l3_lr0001`

### 2. è¨˜éŒ„å…§å®¹
- å§‹çµ‚è¨˜éŒ„è¨“ç·´å’Œé©—è­‰æå¤±
- åŒ…å«é—œéµæŒ‡æ¨™ï¼ˆMAE, MSE, RMSEï¼‰
- è¨˜éŒ„å­¸ç¿’ç‡å’Œè¨“ç·´æ™‚é–“
- é©ç•¶è¨˜éŒ„æ¢¯åº¦ç¯„æ•¸

### 3. æ–‡ä»¶ç®¡ç†
- æ¯å€‹å¯¦é©—ç¨ç«‹ç›®éŒ„
- çµ±ä¸€çš„æ–‡ä»¶å‘½åè¦ç¯„
- å®šæœŸæ¸…ç†èˆŠå¯¦é©—è¨˜éŒ„

### 4. åˆ†ææµç¨‹
- è¨“ç·´å¾Œç«‹å³ç”Ÿæˆè¦–è¦ºåŒ–
- æ¯”è¼ƒé—œéµå¯¦é©—çš„çµæœ
- è¨˜éŒ„å¯¦é©—ç™¼ç¾å’Œæ”¹é€²æ–¹å‘

## ğŸš¨ æ³¨æ„äº‹é …

1. **è¨˜æ†¶é«”ä½¿ç”¨**ï¼šå¤§é‡ epoch è¨˜éŒ„å¯èƒ½ä½”ç”¨è¼ƒå¤šè¨˜æ†¶é«”
2. **æ–‡ä»¶å¤§å°**ï¼šè©³ç´°è¨˜éŒ„æœƒç”¢ç”Ÿè¼ƒå¤§çš„ JSON æ–‡ä»¶
3. **ç‰ˆæœ¬ç›¸å®¹æ€§**ï¼šç¢ºä¿è¼‰å…¥è¨˜éŒ„æ™‚çš„ Python ç‰ˆæœ¬ç›¸å®¹
4. **Git ä¾è³´**ï¼šGit commit è¨˜éŒ„éœ€è¦åœ¨ Git å€‰åº«ä¸­åŸ·è¡Œ

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **ç„¡æ³•è¼‰å…¥è¨˜éŒ„**
   ```python
   # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
   if not Path("training_record.json").exists():
       print("è¨˜éŒ„æ–‡ä»¶ä¸å­˜åœ¨")
   ```

2. **ç¹ªåœ–å¤±æ•—**
   ```python
   # ç¢ºä¿æœ‰ matplotlib å¾Œç«¯
   import matplotlib
   matplotlib.use('Agg')  # ç„¡é¡¯ç¤ºå™¨ç’°å¢ƒ
   ```

3. **è¨˜æ†¶é«”ä¸è¶³**
   ```python
   # æ¸›å°‘è©³ç´°è¨˜éŒ„
   recorder.log_epoch(
       epoch=epoch,
       train_loss=train_loss,
       val_loss=val_loss,
       # ä¸è¨˜éŒ„ sample_predictions
   )
   ```

é€™å€‹ Training Recorder ç³»çµ±ç‚ºä½ çš„å¯¦é©—æä¾›äº†å®Œæ•´çš„è¨˜éŒ„å’Œåˆ†æèƒ½åŠ›ï¼Œç¢ºä¿æ¯å€‹å¯¦é©—éƒ½èƒ½å¾—åˆ°å……åˆ†çš„è¿½è¹¤å’Œç†è§£ã€‚