# ğŸ¯ ç¬¬ä¸€å€‹ Social-xLSTM æ¨¡å‹

**åŸºæ–¼**: å¯¦éš›å¯ç”¨çš„ç¨‹å¼ç¢¼å¯¦ç¾  
**ç›®æ¨™**: 15åˆ†é˜å…§å»ºç«‹ã€è¨“ç·´ä¸¦è©•ä¼°ä½ çš„ç¬¬ä¸€å€‹ Social-xLSTM æ¨¡å‹

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—å°‡å¸¶ä½ å®Œæˆä»¥ä¸‹æ­¥é©Ÿï¼š
1. å»ºç«‹åˆ†æ•£å¼ Social-xLSTM æ¨¡å‹
2. ä½¿ç”¨ç¯„ä¾‹æ•¸æ“šé€²è¡Œè¨“ç·´
3. è©•ä¼°æ¨¡å‹æ€§èƒ½
4. å¯è¦–åŒ–çµæœ

## ğŸ—ï¸ æ¨¡å‹å»ºç«‹

### å‰µå»º `first_model.py`

```python
"""
å®Œæ•´çš„ Social-xLSTM ç¬¬ä¸€å€‹æ¨¡å‹ç¯„ä¾‹
åŸºæ–¼å·²é©—è­‰çš„ç¨‹å¼ç¢¼å¯¦ç¾
"""
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List

# å°å…¥å·²å¯¦ç¾çš„æ¨¡çµ„
from social_xlstm.models.xlstm import TrafficXLSTM, TrafficXLSTMConfig
from social_xlstm.models.social_pooling import SocialPooling, SocialPoolingConfig
from social_xlstm.evaluation.evaluator import ModelEvaluator

class SocialXLSTMModel(nn.Module):
    """
    å®Œæ•´çš„ Social-xLSTM æ¨¡å‹ç¯„ä¾‹
    
    å¯¦ç¾åˆ†æ•£å¼æ¶æ§‹ï¼š
    VD_A: åºåˆ— â†’ xLSTM_A â†’ éš±ç‹€æ…‹_A â”
    VD_B: åºåˆ— â†’ xLSTM_B â†’ éš±ç‹€æ…‹_B â”œâ†’ Social_Pooling â†’ èåˆé æ¸¬
    VD_C: åºåˆ— â†’ xLSTM_C â†’ éš±ç‹€æ…‹_C â”˜
    """
    
    def __init__(self, xlstm_config: TrafficXLSTMConfig, social_config: SocialPoolingConfig):
        super().__init__()
        
        # å…±äº«çš„ xLSTM æ ¸å¿ƒ - æ‰€æœ‰ VD ä½¿ç”¨ç›¸åŒæ¬Šé‡
        self.shared_xlstm = TrafficXLSTM(xlstm_config)
        
        # Social Pooling å±¤ - è™•ç†éš±ç‹€æ…‹
        self.social_pooling = SocialPooling(
            config=social_config,
            feature_dim=xlstm_config.hidden_size  # æ³¨æ„ï¼šä½¿ç”¨éš±ç‹€æ…‹ç¶­åº¦
        )
        
        # èåˆå±¤
        fusion_dim = xlstm_config.hidden_size + social_config.social_embedding_dim
        self.fusion = nn.Linear(fusion_dim, xlstm_config.output_size)
        
        self.xlstm_config = xlstm_config
        self.social_config = social_config
    
    def forward(self, vd_sequences: Dict[str, torch.Tensor], 
                coordinates: torch.Tensor, vd_ids: List[str]) -> Dict[str, torch.Tensor]:
        """
        å‰å‘å‚³æ’­ï¼šåˆ†æ•£å¼ Social-xLSTM
        
        Args:
            vd_sequences: {"VD_001": tensor[batch, seq, features], ...}
            coordinates: tensor[num_vds, 2] - VD åº§æ¨™
            vd_ids: ["VD_001", "VD_002", ...] - VD è­˜åˆ¥ç¢¼
        
        Returns:
            {"VD_001": predictions, ...} - æ¯å€‹ VD çš„é æ¸¬çµæœ
        """
        batch_size = next(iter(vd_sequences.values())).size(0)
        
        # æ­¥é©Ÿ 1: æ¯å€‹ VD ç¨ç«‹çš„ xLSTM è™•ç†
        hidden_states = {}
        for vd_id in vd_ids:
            # ä½¿ç”¨å…±äº«æ¬Šé‡çš„ xLSTM è™•ç†æ¯å€‹ VD çš„åºåˆ—
            xlstm_output = self.shared_xlstm(vd_sequences[vd_id])  # [batch, 1, hidden_size]
            hidden_state = xlstm_output.squeeze(1)  # [batch, hidden_size]
            hidden_states[vd_id] = hidden_state
        
        # æ­¥é©Ÿ 2: å †ç–Šéš±ç‹€æ…‹ç”¨æ–¼ Social Pooling
        hidden_stack = torch.stack([hidden_states[vd_id] for vd_id in vd_ids], dim=1)
        # hidden_stack: [batch, num_vds, hidden_size]
        
        # æ­¥é©Ÿ 3: Social Pooling è™•ç†éš±ç‹€æ…‹
        social_features = self.social_pooling(hidden_stack, coordinates, vd_ids)
        # social_features: [batch, num_vds, social_embedding_dim]
        
        # æ­¥é©Ÿ 4: èåˆé æ¸¬
        predictions = {}
        for i, vd_id in enumerate(vd_ids):
            individual = hidden_stack[:, i, :]     # [batch, hidden_size]
            social = social_features[:, i, :]      # [batch, social_embedding_dim]
            fused = torch.cat([individual, social], dim=-1)  # [batch, fusion_dim]
            pred = self.fusion(fused)              # [batch, output_size]
            predictions[vd_id] = pred
        
        return predictions

# 1. é…ç½®æ¨¡å‹
print("ğŸ”§ é…ç½® Social-xLSTM æ¨¡å‹...")

xlstm_config = TrafficXLSTMConfig(
    input_size=3,
    hidden_size=64,      # é©ä¸­çš„éš±ç‹€æ…‹ç¶­åº¦
    num_blocks=4,        # 4å€‹ xLSTM å¡Š
    output_size=3,
    slstm_at=[1, 3],     # sLSTM åœ¨ä½ç½® 1 å’Œ 3
    sequence_length=12,
    dropout=0.1
)

social_config = SocialPoolingConfig(
    pooling_radius=800.0,           # 800 å…¬å°ºå½±éŸ¿åŠå¾‘
    max_neighbors=3,                # æœ€å¤š 3 å€‹é„°å±…
    social_embedding_dim=24,        # ç¤¾äº¤ç‰¹å¾µç¶­åº¦
    distance_metric="euclidean",
    weighting_function="gaussian",
    aggregation_method="weighted_mean"
)

# 2. å‰µå»ºæ¨¡å‹
model = SocialXLSTMModel(xlstm_config, social_config)
print(f"âœ… æ¨¡å‹å‰µå»ºæˆåŠŸ")
print(f"   xLSTM åƒæ•¸: {sum(p.numel() for p in model.shared_xlstm.parameters()):,}")
print(f"   Social Pooling åƒæ•¸: {sum(p.numel() for p in model.social_pooling.parameters()):,}")
print(f"   ç¸½åƒæ•¸: {sum(p.numel() for p in model.parameters()):,}")
```

## ğŸ² ç¯„ä¾‹æ•¸æ“šç”Ÿæˆ

```python
# 3. æº–å‚™ç¯„ä¾‹æ•¸æ“šï¼ˆæ¨¡æ“¬çœŸå¯¦äº¤é€šæ•¸æ“šï¼‰
print("\\nğŸ“Š æº–å‚™ç¯„ä¾‹æ•¸æ“š...")

def generate_sample_traffic_data():
    """ç”Ÿæˆæ¨¡æ“¬çš„äº¤é€šæ•¸æ“š"""
    # VD åº§æ¨™ï¼ˆå°å—å¸‚å€ç¯„ä¾‹ï¼‰
    coordinates = torch.tensor([
        [120.2062, 22.9908],  # VD_001: å°å—ç«è»Šç«™é™„è¿‘
        [120.2134, 22.9853],  # VD_002: æˆåŠŸå¤§å­¸é™„è¿‘  
        [120.1976, 22.9976],  # VD_003: å®‰å¹³å€
        [120.2095, 22.9845],  # VD_004: æ±å€
    ], dtype=torch.float32)
    
    vd_ids = ["VD_001", "VD_002", "VD_003", "VD_004"]
    
    # ç”Ÿæˆæ™‚é–“åºåˆ—æ•¸æ“š (æ‰¹æ¬¡å¤§å°=8, åºåˆ—é•·åº¦=12, ç‰¹å¾µ=3)
    batch_size, seq_len, num_features = 8, 12, 3
    vd_sequences = {}
    
    for i, vd_id in enumerate(vd_ids):
        # æ¨¡æ“¬ä¸åŒ VD çš„äº¤é€šæ¨¡å¼
        base_traffic = 50 + i * 10  # ä¸åŒåŸºç¤æµé‡
        
        # æ·»åŠ æ™‚é–“è¶¨å‹¢å’Œéš¨æ©Ÿè®ŠåŒ–
        time_trend = torch.linspace(-0.5, 0.5, seq_len).unsqueeze(0).repeat(batch_size, 1)
        sequences = []
        
        for batch in range(batch_size):
            sequence = []
            for t in range(seq_len):
                # é€Ÿåº¦ (km/h): åŸºç¤é€Ÿåº¦ + æ™‚é–“è¶¨å‹¢ + å™ªéŸ³
                speed = base_traffic + time_trend[batch, t] * 10 + torch.randn(1) * 5
                speed = torch.clamp(speed, 10, 100)  # é™åˆ¶åœ¨åˆç†ç¯„åœ
                
                # æµé‡ (vehicles/hour): èˆ‡é€Ÿåº¦ç›¸é—œ
                volume = 1000 - speed * 8 + torch.randn(1) * 100
                volume = torch.clamp(volume, 100, 2000)
                
                # ä½”æœ‰ç‡ (%): èˆ‡æµé‡ç›¸é—œ
                occupancy = volume / 30 + torch.randn(1) * 3
                occupancy = torch.clamp(occupancy, 5, 90)
                
                sequence.append([speed.item(), volume.item(), occupancy.item()])
            
            sequences.append(sequence)
        
        vd_sequences[vd_id] = torch.tensor(sequences, dtype=torch.float32)
    
    return vd_sequences, coordinates, vd_ids

vd_sequences, coordinates, vd_ids = generate_sample_traffic_data()

# é¡¯ç¤ºæ•¸æ“šè³‡è¨Š
print(f"âœ… æ•¸æ“šç”Ÿæˆå®Œæˆ")
print(f"   VD æ•¸é‡: {len(vd_ids)}")
print(f"   æ‰¹æ¬¡å¤§å°: {vd_sequences[vd_ids[0]].shape[0]}")
print(f"   åºåˆ—é•·åº¦: {vd_sequences[vd_ids[0]].shape[1]}")
print(f"   ç‰¹å¾µæ•¸é‡: {vd_sequences[vd_ids[0]].shape[2]}")

# é¡¯ç¤ºç¯„ä¾‹æ•¸æ“š
print(f"\\nğŸ“‹ VD_001 ç¬¬ä¸€å€‹æ‰¹æ¬¡çš„å‰3å€‹æ™‚é–“æ­¥:")
sample_data = vd_sequences["VD_001"][0, :3, :]
for t, (speed, volume, occupancy) in enumerate(sample_data):
    print(f"   t={t}: é€Ÿåº¦={speed:.1f}km/h, æµé‡={volume:.0f}è¼›/h, ä½”æœ‰ç‡={occupancy:.1f}%")
```

## ğŸš€ æ¨¡å‹è¨“ç·´

```python
# 4. ç°¡å–®è¨“ç·´å¾ªç’°
print("\\nğŸš€ é–‹å§‹è¨“ç·´...")

# è¨­ç½®è¨“ç·´åƒæ•¸
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()
num_epochs = 20

model.train()
losses = []

for epoch in range(num_epochs):
    optimizer.zero_grad()
    
    # å‰å‘å‚³æ’­
    predictions = model(vd_sequences, coordinates, vd_ids)
    
    # è¨ˆç®—æå¤±ï¼ˆç°¡å–®çš„è‡ªå›æ­¸ä»»å‹™ï¼šé æ¸¬ä¸‹ä¸€æ™‚åˆ»çš„å€¼ï¼‰
    total_loss = 0
    for vd_id in vd_ids:
        # ä½¿ç”¨æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥ä½œç‚ºç›®æ¨™
        target = vd_sequences[vd_id][:, -1, :]  # [batch, features]
        pred = predictions[vd_id]  # [batch, features]
        loss = criterion(pred, target)
        total_loss += loss
    
    # åå‘å‚³æ’­
    total_loss.backward()
    optimizer.step()
    
    losses.append(total_loss.item())
    
    if (epoch + 1) % 5 == 0:
        print(f"   Epoch {epoch+1:2d}/{num_epochs}, Loss: {total_loss.item():.4f}")

print(f"âœ… è¨“ç·´å®Œæˆï¼Œæœ€çµ‚æå¤±: {losses[-1]:.4f}")
```

## ğŸ“Š æ¨¡å‹è©•ä¼°

```python
# 5. è©•ä¼°æ¨¡å‹
print("\\nğŸ“Š è©•ä¼°æ¨¡å‹æ€§èƒ½...")

model.eval()
evaluator = ModelEvaluator()

with torch.no_grad():
    predictions = model(vd_sequences, coordinates, vd_ids)
    
    # è©•ä¼°æ¯å€‹ VD
    all_metrics = {}
    for vd_id in vd_ids:
        target = vd_sequences[vd_id][:, -1, :].numpy()  # [batch, features]
        pred = predictions[vd_id].numpy()  # [batch, features]
        
        # åˆ†åˆ¥è©•ä¼°æ¯å€‹ç‰¹å¾µï¼ˆé€Ÿåº¦ã€æµé‡ã€ä½”æœ‰ç‡ï¼‰
        feature_names = ["é€Ÿåº¦", "æµé‡", "ä½”æœ‰ç‡"]
        vd_metrics = {}
        
        for i, feature_name in enumerate(feature_names):
            metrics = evaluator.compute_all_metrics(
                y_true=target[:, i], 
                y_pred=pred[:, i]
            )
            vd_metrics[feature_name] = metrics
        
        all_metrics[vd_id] = vd_metrics

# é¡¯ç¤ºè©•ä¼°çµæœ
print("\\nğŸ“ˆ æ¨¡å‹æ€§èƒ½è©•ä¼°çµæœ:")
print("=" * 60)

for vd_id in vd_ids:
    print(f"\\nğŸš— {vd_id}:")
    for feature_name in ["é€Ÿåº¦", "æµé‡", "ä½”æœ‰ç‡"]:
        metrics = all_metrics[vd_id][feature_name]
        print(f"   {feature_name:>4s}: MAE={metrics['mae']:.2f}, "
              f"RMSE={metrics['rmse']:.2f}, RÂ²={metrics['r2']:.3f}")

# è¨ˆç®—å¹³å‡æ€§èƒ½
print("\\nğŸ¯ æ•´é«”å¹³å‡æ€§èƒ½:")
avg_mae = np.mean([[all_metrics[vd][feat]['mae'] 
                   for feat in ["é€Ÿåº¦", "æµé‡", "ä½”æœ‰ç‡"]] 
                   for vd in vd_ids])
avg_rmse = np.mean([[all_metrics[vd][feat]['rmse'] 
                    for feat in ["é€Ÿåº¦", "æµé‡", "ä½”æœ‰ç‡"]] 
                    for vd in vd_ids])
avg_r2 = np.mean([[all_metrics[vd][feat]['r2'] 
                  for feat in ["é€Ÿåº¦", "æµé‡", "ä½”æœ‰ç‡"]] 
                  for vd in vd_ids])

print(f"   å¹³å‡ MAE: {avg_mae:.2f}")
print(f"   å¹³å‡ RMSE: {avg_rmse:.2f}")
print(f"   å¹³å‡ RÂ²: {avg_r2:.3f}")
```

## ğŸ“ˆ çµæœå¯è¦–åŒ–

```python
# 6. ç°¡å–®çš„çµæœå¯è¦–åŒ–
print("\\nğŸ“ˆ ç”Ÿæˆå¯è¦–åŒ–çµæœ...")

import matplotlib.pyplot as plt

# ç¹ªè£½è¨“ç·´æå¤±
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(losses)
plt.title('è¨“ç·´æå¤±æ›²ç·š')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)

# ç¹ªè£½é æ¸¬ vs çœŸå¯¦å€¼ç¤ºä¾‹ï¼ˆVD_001 çš„é€Ÿåº¦ï¼‰
plt.subplot(1, 2, 2)
vd_id = "VD_001"
target_speed = vd_sequences[vd_id][:, -1, 0].numpy()  # å¯¦éš›é€Ÿåº¦
pred_speed = predictions[vd_id][:, 0].detach().numpy()  # é æ¸¬é€Ÿåº¦

plt.scatter(target_speed, pred_speed, alpha=0.7)
plt.plot([target_speed.min(), target_speed.max()], 
         [target_speed.min(), target_speed.max()], 'r--', alpha=0.8)
plt.xlabel('å¯¦éš›é€Ÿåº¦ (km/h)')
plt.ylabel('é æ¸¬é€Ÿåº¦ (km/h)')
plt.title(f'{vd_id} é€Ÿåº¦é æ¸¬æº–ç¢ºåº¦')
plt.grid(True)

plt.tight_layout()
plt.savefig('first_model_results.png', dpi=150, bbox_inches='tight')
print("âœ… çµæœå·²ä¿å­˜åˆ° 'first_model_results.png'")

# é¡¯ç¤º Social Pooling çš„é„°å±…é—œä¿‚
print("\\nğŸŒ Social Pooling é„°å±…é—œä¿‚åˆ†æ:")
neighbor_info = model.social_pooling.get_neighbor_info(coordinates, vd_ids, node_idx=0)
print(f"   {neighbor_info['node_id']} çš„é„°å±…:")
for i, (neighbor_id, distance, weight) in enumerate(zip(
    neighbor_info['neighbor_ids'], 
    neighbor_info['neighbor_distances'],
    neighbor_info['neighbor_weights']
)):
    print(f"     {neighbor_id}: è·é›¢={distance:.0f}m, æ¬Šé‡={weight:.3f}")

print("\\nğŸ‰ ç¬¬ä¸€å€‹ Social-xLSTM æ¨¡å‹å®Œæˆï¼")
print("\\nğŸ“‹ æ¨¡å‹æ‘˜è¦:")
print(f"   æ¶æ§‹: åˆ†æ•£å¼ Social-xLSTM")
print(f"   xLSTM å¡Šæ•¸: {xlstm_config.num_blocks}")
print(f"   ç¤¾äº¤åŠå¾‘: {social_config.pooling_radius}m")
print(f"   ç¸½åƒæ•¸: {sum(p.numel() for p in model.parameters()):,}")
print(f"   å¹³å‡ RÂ²: {avg_r2:.3f}")
```

## ğŸ¯ é‹è¡Œå®Œæ•´ç¯„ä¾‹

å°‡æ‰€æœ‰ç¨‹å¼ç¢¼ä¿å­˜åˆ° `first_model.py`ï¼Œç„¶å¾Œé‹è¡Œï¼š

```bash
python first_model.py
```

**é æœŸè¼¸å‡ºçµæ§‹**ï¼š
```
ğŸ”§ é…ç½® Social-xLSTM æ¨¡å‹...
âœ… æ¨¡å‹å‰µå»ºæˆåŠŸ
   xLSTM åƒæ•¸: 143,891
   ...

ğŸ“Š æº–å‚™ç¯„ä¾‹æ•¸æ“š...
âœ… æ•¸æ“šç”Ÿæˆå®Œæˆ
   ...

ğŸš€ é–‹å§‹è¨“ç·´...
   Epoch  5/20, Loss: 2485.4839
   ...

ğŸ“Š è©•ä¼°æ¨¡å‹æ€§èƒ½...
ğŸ“ˆ æ¨¡å‹æ€§èƒ½è©•ä¼°çµæœ:
...

ğŸ‰ ç¬¬ä¸€å€‹ Social-xLSTM æ¨¡å‹å®Œæˆï¼
```

## ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°

å®Œæˆç¬¬ä¸€å€‹æ¨¡å‹å¾Œï¼Œå»ºè­°æ¢ç´¢ï¼š

1. **çœŸå¯¦æ•¸æ“šè¨“ç·´**: [å®Œæ•´è¨“ç·´æŒ‡å—](../guides/training-guide.md)
2. **æ¨¡å‹é…ç½®èª¿å„ª**: [æ¨¡å‹é…ç½®æŒ‡å—](../guides/model-configuration.md)
3. **é«˜ç´š Social Pooling**: [Social Pooling é€²éšç”¨æ³•](../guides/social-pooling-advanced.md)
4. **æ€§èƒ½åŸºæº–æ¯”è¼ƒ**: [æ€§èƒ½åŸºæº–](../reference/benchmarks.md)

## ğŸ’¡ é—œéµè¦é»

- âœ… **åˆ†æ•£å¼æ¶æ§‹**: æ¯å€‹ VD ç¨ç«‹çš„ xLSTM æ ¸å¿ƒ
- âœ… **éš±ç‹€æ…‹èšåˆ**: Social Pooling ä½œç”¨æ–¼é«˜å±¤ç‰¹å¾µ
- âœ… **æ¬Šé‡å…±äº«**: æ‰€æœ‰ VD å…±äº«ç›¸åŒçš„ xLSTM æ¬Šé‡
- âœ… **åº§æ¨™é©…å‹•**: åŸºæ–¼åœ°ç†ä½ç½®çš„ç©ºé–“èšåˆ

é€™å€‹ç¯„ä¾‹å±•ç¤ºäº† Social-xLSTM çš„æ ¸å¿ƒåŸç†ï¼Œç‚ºé€²ä¸€æ­¥çš„ç ”ç©¶å’Œæ‡‰ç”¨å¥ å®šäº†åŸºç¤ã€‚