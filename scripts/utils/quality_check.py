#!/usr/bin/env python3
"""
äº¤é€šè³‡æ–™å“è³ªæª¢æŸ¥å·¥å…·

ä½¿ç”¨æ–¹å¼:
    python scripts/utils/quality_check.py --h5_file blob/dataset/pre-processed/h5/traffic_features.h5
    python scripts/utils/quality_check.py --json_dir blob/dataset/pre-processed/unzip_to_json --sample_size 10
    python scripts/utils/quality_check.py --all  # æª¢æŸ¥æ‰€æœ‰
"""

import argparse
import h5py
import numpy as np
import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

from social_xlstm.dataset.utils.json_utils import VDLiveList


def detect_error_codes(data, feature_name):
    """æª¢æ¸¬ä¸¦å›å ±éŒ¯èª¤ç¢¼ã€‚"""
    if len(data) == 0:
        return {}
        
    # å¸¸è¦‹éŒ¯èª¤ç¢¼
    error_codes = [-99, -1, 255, 999]
    error_stats = {}
    
    for code in error_codes:
        count = np.sum(data == code)
        if count > 0:
            error_stats[code] = {
                'count': count,
                'percentage': count / data.size * 100
            }
    
    # æª¢æŸ¥ç•°å¸¸ç¯„åœ
    if feature_name == 'speed':
        invalid_count = np.sum((data < -200) | (data > 300))
    elif feature_name == 'occupancy':
        invalid_count = np.sum((data < -200) | (data > 200))
    else:
        invalid_count = np.sum((data < -1000) | (data > 1000))
    
    if invalid_count > 0:
        error_stats['out_of_range'] = {
            'count': invalid_count,
            'percentage': invalid_count / data.size * 100
        }
    
    return error_stats


def comprehensive_h5_quality_check(h5_file_path):
    """å…¨é¢çš„ H5 æª”æ¡ˆå“è³ªæª¢æŸ¥ã€‚"""
    
    if not os.path.exists(h5_file_path):
        print(f"âŒ H5 æª”æ¡ˆä¸å­˜åœ¨: {h5_file_path}")
        return
    
    try:
        with h5py.File(h5_file_path, 'r') as f:
            features = f['data/features']
            feature_names = [name.decode() for name in f['metadata/feature_names']]
            timestamps = [ts.decode() for ts in f['metadata/timestamps']]
            
            print(f"=== H5 æª”æ¡ˆå“è³ªå ±å‘Š: {h5_file_path} ===")
            print(f"è³‡æ–™ç¶­åº¦: {features.shape} (æ™‚é–“æ­¥, VDæ•¸, ç‰¹å¾µæ•¸)")
            print(f"æ™‚é–“ç¯„åœ: {timestamps[0]} åˆ° {timestamps[-1]}")
            print(f"ç‰¹å¾µ: {feature_names}")
            
            # æ•´é«”çµ±è¨ˆ
            total_values = features.size
            nan_values = np.sum(np.isnan(features))
            valid_values = total_values - nan_values
            
            print(f"\n=== æ•´é«”çµ±è¨ˆ ===")
            print(f"ç¸½æ•¸å€¼: {total_values:,}")
            print(f"æœ‰æ•ˆå€¼: {valid_values:,} ({valid_values/total_values:.1%})")
            print(f"NaN å€¼: {nan_values:,} ({nan_values/total_values:.1%})")
            
            # å„ç‰¹å¾µçµ±è¨ˆ
            print(f"\n=== å„ç‰¹å¾µå“è³ª ===")
            anomaly_total = 0
            
            for i, fname in enumerate(feature_names):
                feature_data = features[:, :, i]
                valid_data = feature_data[~np.isnan(feature_data)]
                
                print(f"\n{fname}:")
                print(f"  æœ‰æ•ˆå€¼: {len(valid_data):,}/{feature_data.size:,} ({len(valid_data)/feature_data.size:.1%})")
                
                if len(valid_data) > 0:
                    print(f"  ç¯„åœ: {valid_data.min():.2f} - {valid_data.max():.2f}")
                    print(f"  å¹³å‡: {valid_data.mean():.2f}")
                    print(f"  æ¨™æº–å·®: {valid_data.std():.2f}")
                    print(f"  ä¸­ä½æ•¸: {np.median(valid_data):.2f}")
                    
                    # ç•°å¸¸å€¼æª¢æŸ¥
                    anomalies = 0
                    if fname == 'avg_speed':
                        anomalies = np.sum((valid_data < 0) | (valid_data > 200))
                    elif fname == 'avg_occupancy':
                        anomalies = np.sum((valid_data < 0) | (valid_data > 100))
                    elif fname in ['total_volume', 'lane_count']:
                        anomalies = np.sum(valid_data < 0)
                    elif fname == 'speed_std':
                        anomalies = np.sum(valid_data < 0)
                    
                    anomaly_total += anomalies
                    
                    if anomalies > 0:
                        print(f"  âš ï¸ ç•°å¸¸å€¼: {anomalies} ({anomalies/len(valid_data):.1%})")
                    else:
                        print(f"  âœ… ç„¡ç•°å¸¸å€¼")
                        
                    # åˆ†å¸ƒçµ±è¨ˆ
                    q25, q75 = np.percentile(valid_data, [25, 75])
                    print(f"  å››åˆ†ä½ç¯„åœ: {q25:.2f} - {q75:.2f}")
                        
                else:
                    print(f"  âŒ ç„¡æœ‰æ•ˆè³‡æ–™")
            
            # VD è¦†è“‹ç‡åˆ†æ
            print(f"\n=== VD è¦†è“‹ç‡åˆ†æ ===")
            locations_with_data = 0
            vd_data_quality = []
            
            for vd_idx in range(features.shape[1]):
                vd_data = features[:, vd_idx, :]
                valid_count = np.sum(~np.isnan(vd_data))
                total_count = vd_data.size
                
                if valid_count > 0:
                    locations_with_data += 1
                    quality_ratio = valid_count / total_count
                    vd_data_quality.append(quality_ratio)
            
            coverage = locations_with_data / features.shape[1]
            print(f"æœ‰è³‡æ–™çš„ VD: {locations_with_data}/{features.shape[1]} ({coverage:.1%})")
            
            if vd_data_quality:
                avg_vd_quality = np.mean(vd_data_quality)
                print(f"VD å¹³å‡è³‡æ–™å®Œæ•´æ€§: {avg_vd_quality:.1%}")
                print(f"VD è³‡æ–™å®Œæ•´æ€§ç¯„åœ: {np.min(vd_data_quality):.1%} - {np.max(vd_data_quality):.1%}")
            
            # æ™‚é–“å®Œæ•´æ€§åˆ†æ
            print(f"\n=== æ™‚é–“å®Œæ•´æ€§åˆ†æ ===")
            time_completeness = []
            time_quality_by_feature = {fname: [] for fname in feature_names}
            
            for t in range(features.shape[0]):
                timestep_data = features[t, :, :]
                valid_ratio = np.sum(~np.isnan(timestep_data)) / timestep_data.size
                time_completeness.append(valid_ratio)
                
                # å„ç‰¹å¾µçš„æ™‚é–“å“è³ª
                for i, fname in enumerate(feature_names):
                    feature_slice = features[t, :, i]
                    feature_valid_ratio = np.sum(~np.isnan(feature_slice)) / feature_slice.size
                    time_quality_by_feature[fname].append(feature_valid_ratio)
            
            avg_completeness = np.mean(time_completeness)
            min_completeness = np.min(time_completeness)
            max_completeness = np.max(time_completeness)
            
            print(f"å¹³å‡æ™‚é–“å®Œæ•´æ€§: {avg_completeness:.1%}")
            print(f"æ™‚é–“å®Œæ•´æ€§ç¯„åœ: {min_completeness:.1%} - {max_completeness:.1%}")
            
            # å„ç‰¹å¾µæ™‚é–“ç©©å®šæ€§
            print(f"\n=== ç‰¹å¾µæ™‚é–“ç©©å®šæ€§ ===")
            for fname in feature_names:
                quality_values = time_quality_by_feature[fname]
                std_quality = np.std(quality_values)
                print(f"{fname}: å¹³å‡ {np.mean(quality_values):.1%}, æ¨™æº–å·® {std_quality:.3f}")
            
            # å“è³ªç­‰ç´šè©•ä¼°
            print(f"\n=== å“è³ªè©•ä¼° ===")
            quality_score = 0
            max_score = 100
            
            # VD è¦†è“‹ç‡è©•åˆ† (25åˆ†)
            if coverage >= 0.9:
                quality_score += 25
                print("âœ… VD è¦†è“‹ç‡: å„ªç§€ (â‰¥90%)")
            elif coverage >= 0.8:
                quality_score += 18
                print("âš ï¸ VD è¦†è“‹ç‡: è‰¯å¥½ (80-90%)")
            elif coverage >= 0.7:
                quality_score += 12
                print("âš ï¸ VD è¦†è“‹ç‡: æ™®é€š (70-80%)")
            else:
                quality_score += 5
                print("âŒ VD è¦†è“‹ç‡: éœ€æ”¹å–„ (<70%)")
            
            # æœ‰æ•ˆè³‡æ–™æ¯”ä¾‹è©•åˆ† (25åˆ†)
            valid_ratio = valid_values/total_values
            if valid_ratio >= 0.85:
                quality_score += 25
                print("âœ… æœ‰æ•ˆè³‡æ–™æ¯”ä¾‹: å„ªç§€ (â‰¥85%)")
            elif valid_ratio >= 0.7:
                quality_score += 18
                print("âš ï¸ æœ‰æ•ˆè³‡æ–™æ¯”ä¾‹: è‰¯å¥½ (70-85%)")
            elif valid_ratio >= 0.6:
                quality_score += 12
                print("âš ï¸ æœ‰æ•ˆè³‡æ–™æ¯”ä¾‹: æ™®é€š (60-70%)")
            else:
                quality_score += 5
                print("âŒ æœ‰æ•ˆè³‡æ–™æ¯”ä¾‹: éœ€æ”¹å–„ (<60%)")
            
            # æ™‚é–“å®Œæ•´æ€§è©•åˆ† (25åˆ†)
            if avg_completeness >= 0.95:
                quality_score += 25
                print("âœ… æ™‚é–“å®Œæ•´æ€§: å„ªç§€ (â‰¥95%)")
            elif avg_completeness >= 0.85:
                quality_score += 18
                print("âš ï¸ æ™‚é–“å®Œæ•´æ€§: è‰¯å¥½ (85-95%)")
            elif avg_completeness >= 0.75:
                quality_score += 12
                print("âš ï¸ æ™‚é–“å®Œæ•´æ€§: æ™®é€š (75-85%)")
            else:
                quality_score += 5
                print("âŒ æ™‚é–“å®Œæ•´æ€§: éœ€æ”¹å–„ (<75%)")
            
            # ç•°å¸¸å€¼è©•åˆ† (25åˆ†)
            anomaly_ratio = anomaly_total / valid_values if valid_values > 0 else 1
            if anomaly_ratio == 0:
                quality_score += 25
                print("âœ… ç„¡ç•°å¸¸å€¼: å„ªç§€")
            elif anomaly_ratio < 0.01:
                quality_score += 18
                print("âš ï¸ ç•°å¸¸å€¼: è‰¯å¥½ (<1%)")
            elif anomaly_ratio < 0.05:
                quality_score += 12
                print("âš ï¸ ç•°å¸¸å€¼: æ™®é€š (1-5%)")
            else:
                quality_score += 5
                print("âŒ ç•°å¸¸å€¼: éœ€æ”¹å–„ (>5%)")
            
            print(f"\n=== ç¸½é«”å“è³ªè©•åˆ†: {quality_score}/{max_score} ===")
            if quality_score >= 85:
                print("ğŸ‰ è³‡æ–™å“è³ªå„ªç§€ï¼Œå¯ç›´æ¥ç”¨æ–¼è¨“ç·´")
            elif quality_score >= 70:
                print("âš ï¸ è³‡æ–™å“è³ªè‰¯å¥½ï¼Œå»ºè­°é€²ä¸€æ­¥æ¸…ç†")
            elif quality_score >= 50:
                print("âš ï¸ è³‡æ–™å“è³ªæ™®é€šï¼Œéœ€è¦æ¸…ç†å’Œé è™•ç†")
            else:
                print("âŒ è³‡æ–™å“è³ªä¸ä½³ï¼Œéœ€è¦é‡æ–°è™•ç†")
                
            # å»ºè­°
            print(f"\n=== æ”¹å–„å»ºè­° ===")
            if coverage < 0.8:
                print("â€¢ æª¢æŸ¥ VD åŒ¹é…é‚è¼¯ï¼Œç¢ºèª VDList å’Œ VDLiveList åŒæ­¥")
            if valid_ratio < 0.7:
                print("â€¢ æª¢æŸ¥éŒ¯èª¤ç¢¼éæ¿¾é‚è¼¯ï¼Œå¯èƒ½æœ‰æ–°çš„éŒ¯èª¤æ¨¡å¼")
            if avg_completeness < 0.8:
                print("â€¢ æª¢æŸ¥æ™‚é–“åºåˆ—å®Œæ•´æ€§ï¼Œè€ƒæ…®æ™‚é–“é–“éš”æ’å€¼")
            if anomaly_ratio > 0.01:
                print("â€¢ åŠ å¼·ç•°å¸¸å€¼æª¢æ¸¬ï¼Œæª¢æŸ¥ç‰¹å¾µæå–é‚è¼¯")
                
    except Exception as e:
        print(f"âŒ æª¢æŸ¥ H5 æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def check_json_quality(json_dir_path, sample_size=None):
    """æª¢æŸ¥åŸå§‹ JSON è³‡æ–™å“è³ªã€‚"""
    
    if not os.path.exists(json_dir_path):
        print(f"âŒ JSON ç›®éŒ„ä¸å­˜åœ¨: {json_dir_path}")
        return
    
    time_dirs = sorted([d for d in os.listdir(json_dir_path) 
                       if os.path.isdir(os.path.join(json_dir_path, d))])
    
    if not time_dirs:
        print(f"âŒ JSON ç›®éŒ„ä¸­æ²’æœ‰æ™‚é–“ç›®éŒ„: {json_dir_path}")
        return
    
    print(f"=== JSON è³‡æ–™å“è³ªæª¢æŸ¥: {json_dir_path} ===")
    print(f"æ™‚é–“ç›®éŒ„æ•¸é‡: {len(time_dirs)}")
    
    # æ±ºå®šæ¨£æœ¬å¤§å°
    if sample_size is None:
        sample_size = min(10, len(time_dirs))
    else:
        sample_size = min(sample_size, len(time_dirs))
    
    # æŠ½æ¨£æª¢æŸ¥
    if len(time_dirs) > sample_size:
        step = len(time_dirs) // sample_size
        sample_dirs = time_dirs[::step][:sample_size]
        print(f"æŠ½æ¨£æª¢æŸ¥ {len(sample_dirs)} å€‹æ™‚é–“ç›®éŒ„ (æ¯ {step} å€‹å–ä¸€å€‹)")
    else:
        sample_dirs = time_dirs
        print(f"æª¢æŸ¥æ‰€æœ‰ {len(sample_dirs)} å€‹æ™‚é–“ç›®éŒ„")
    
    error_code_stats = {}
    vd_count_stats = []
    data_availability_stats = []
    processing_errors = []
    
    for i, time_dir in enumerate(sample_dirs):
        print(f"è™•ç† {i+1}/{len(sample_dirs)}: {time_dir}", end=" ... ")
        
        vd_live_file = os.path.join(json_dir_path, time_dir, 'VDLiveList.json')
        
        if not os.path.exists(vd_live_file):
            print("âŒ VDLiveList.json ä¸å­˜åœ¨")
            processing_errors.append(f"{time_dir}: VDLiveList.json ä¸å­˜åœ¨")
            continue
        
        try:
            vd_live_list = VDLiveList.load_from_json(vd_live_file)
            vd_count = len(vd_live_list.LiveTrafficData)
            vd_count_stats.append(vd_count)
            
            vds_with_data = 0
            speed_values = []
            occupancy_values = []
            volume_values = []
            
            for vd in vd_live_list.LiveTrafficData:
                has_data = False
                if vd.LinkFlows:
                    for link in vd.LinkFlows:
                        if link.Lanes:
                            has_data = True
                            for lane in link.Lanes:
                                if lane.Speed is not None:
                                    speed_values.append(lane.Speed)
                                if lane.Occupancy is not None:
                                    occupancy_values.append(lane.Occupancy)
                                if lane.Vehicles:
                                    for vehicle in lane.Vehicles:
                                        if vehicle.Volume is not None:
                                            volume_values.append(vehicle.Volume)
                
                if has_data:
                    vds_with_data += 1
            
            availability = vds_with_data / vd_count if vd_count > 0 else 0
            data_availability_stats.append(availability)
            
            # éŒ¯èª¤ç¢¼çµ±è¨ˆ
            for values, name in [(speed_values, 'speed'), 
                               (occupancy_values, 'occupancy'), 
                               (volume_values, 'volume')]:
                if values:
                    error_stats = detect_error_codes(np.array(values), name)
                    for code, stats in error_stats.items():
                        key = f"{name}_{code}"
                        if key not in error_code_stats:
                            error_code_stats[key] = []
                        error_code_stats[key].append(stats['percentage'])
                        
            print("âœ…")
                    
        except Exception as e:
            print(f"âŒ éŒ¯èª¤: {e}")
            processing_errors.append(f"{time_dir}: {e}")
    
    # å ±å‘Šçµæœ
    if vd_count_stats:
        print(f"\n=== VD æ•¸é‡çµ±è¨ˆ ===")
        print(f"å¹³å‡ VD æ•¸é‡: {np.mean(vd_count_stats):.0f}")
        print(f"VD æ•¸é‡ç¯„åœ: {np.min(vd_count_stats)} - {np.max(vd_count_stats)}")
        print(f"VD æ•¸é‡æ¨™æº–å·®: {np.std(vd_count_stats):.1f}")
    
    if data_availability_stats:
        print(f"\n=== è³‡æ–™å¯ç”¨æ€§ ===")
        print(f"å¹³å‡æœ‰è³‡æ–™çš„ VD æ¯”ä¾‹: {np.mean(data_availability_stats):.1%}")
        print(f"å¯ç”¨æ€§ç¯„åœ: {np.min(data_availability_stats):.1%} - {np.max(data_availability_stats):.1%}")
        print(f"å¯ç”¨æ€§æ¨™æº–å·®: {np.std(data_availability_stats):.3f}")
    
    if error_code_stats:
        print(f"\n=== éŒ¯èª¤ç¢¼çµ±è¨ˆ ===")
        for key, percentages in error_code_stats.items():
            if percentages:  # ç¢ºä¿åˆ—è¡¨ä¸ç‚ºç©º
                avg_percentage = np.mean(percentages)
                if avg_percentage > 0.1:  # åªé¡¯ç¤º >0.1% çš„éŒ¯èª¤
                    min_pct = np.min(percentages)
                    max_pct = np.max(percentages)
                    print(f"{key}: å¹³å‡ {avg_percentage:.1%} (ç¯„åœ: {min_pct:.1%}-{max_pct:.1%})")
    
    if processing_errors:
        print(f"\n=== è™•ç†éŒ¯èª¤ ({len(processing_errors)}) ===")
        for error in processing_errors[:10]:  # åªé¡¯ç¤ºå‰10å€‹éŒ¯èª¤
            print(f"â€¢ {error}")
        if len(processing_errors) > 10:
            print(f"... é‚„æœ‰ {len(processing_errors) - 10} å€‹éŒ¯èª¤")


def main():
    parser = argparse.ArgumentParser(description="äº¤é€šè³‡æ–™å“è³ªæª¢æŸ¥å·¥å…·")
    parser.add_argument("--h5_file", type=str, help="H5 æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--json_dir", type=str, help="JSON ç›®éŒ„è·¯å¾‘")
    parser.add_argument("--sample_size", type=int, default=10, help="JSON æª¢æŸ¥çš„æ¨£æœ¬å¤§å°")
    parser.add_argument("--all", action="store_true", help="æª¢æŸ¥æ‰€æœ‰é è¨­æª”æ¡ˆ")
    
    args = parser.parse_args()
    
    if args.all:
        # æª¢æŸ¥é è¨­æª”æ¡ˆ
        default_h5 = "blob/dataset/pre-processed/h5/traffic_features.h5"
        default_json = "blob/dataset/pre-processed/unzip_to_json"
        
        if os.path.exists(default_h5):
            comprehensive_h5_quality_check(default_h5)
            print("\n" + "="*80 + "\n")
        
        if os.path.exists(default_json):
            check_json_quality(default_json, args.sample_size)
    
    else:
        if args.h5_file:
            comprehensive_h5_quality_check(args.h5_file)
        
        if args.json_dir:
            if args.h5_file:
                print("\n" + "="*80 + "\n")
            check_json_quality(args.json_dir, args.sample_size)
    
    if not (args.h5_file or args.json_dir or args.all):
        parser.print_help()


if __name__ == "__main__":
    main()