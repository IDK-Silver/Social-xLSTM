# Generic rule logging configuration
generic_logs:
  reports: "logs/dev/reports/generate_training_report_{experiment_path}.log"
  plots: "logs/dev/reports/generate_training_plots_{experiment_path}.log"

storage:
  cold_storage:
    raw_zip:
      type: zip
      folders:
      - /home/GP/dataset/collect/cold_storage
dataset:
  pre-processed:
    raw_zip_list:
      type: txt-list
      log: logs/dev/pre-processed/raw_zip_list.log
      file: blob/dataset/pre-processed/raw_zip_list.txt
    unzip_to_json:
      type: json
      status: blob/status/dev_unzip_to_json.json
      log: logs/dev/pre-processed/unzip_to_json.log
      folder: blob/dataset/pre-processed/unzip_to_json
    h5:
      file: blob/dataset/pre-processed/h5/traffic_features_dev.h5
      log: logs/dev/create_h5_file.log
      selected_vdids:
      - VD-28-0740-000-001
      # Time range for data processing (Format: YYYY-MM-DD_HH-MM-SS,YYYY-MM-DD_HH-MM-SS)
      # Set to `null` to process all available data without filtering
      # See docs/implementation/dataset_documentation.md for detailed usage
      time_range: 2025-03-18_00-00-00,2025-03-18_23-59-59
      overwrite: true

training_lstm:
  single_vd:
    experiment_dir: blob/experiments/dev/lstm/single_vd
    log: logs/dev/train_single_vd_lstm.log
    report_log: "logs/dev/reports/generate_single_vd_report.log"  # Report generation log
    plot_log: "logs/dev/reports/generate_single_vd_plots.log"    # Plot generation log
    epochs: 10
    batch_size: 32
    sequence_length: 5
    model_type: lstm
    select_vd_id: VD-28-0740-000-001
    hidden_size: 32
    num_layers: 1
    dropout: 0.5
    learning_rate: 0.0005
    weight_decay: 0.01
    # Optimized training parameters from fixed configs
    early_stopping_patience: 8
    gradient_clip_value: 0.5
    use_scheduler: true
    scheduler_patience: 5
    # Dataset parameters
    prediction_length: 1
    train_ratio: 0.8
    val_ratio: 0.2
    test_ratio: 0.0
    normalize: true
    normalization_method: standard
  multi_vd:
    experiment_dir: blob/experiments/dev/lstm/multi_vd
    log: logs/dev/train_multi_vd_lstm.log
    report_log: "logs/dev/reports/generate_multi_vd_report.log"  # Report generation log
    plot_log: "logs/dev/reports/generate_multi_vd_plots.log"    # Plot generation log
    epochs: 10
    batch_size: 32
    sequence_length: 5
    num_vds: 1
    model_type: lstm
    hidden_size: 32
    num_layers: 1
    dropout: 0.5
    learning_rate: 0.0005
    weight_decay: 0.01
    # Optimized training parameters from fixed configs
    early_stopping_patience: 8
    gradient_clip_value: 0.5
    use_scheduler: true
    scheduler_patience: 5
    # Dataset parameters
    prediction_length: 1
    train_ratio: 0.8
    val_ratio: 0.2
    test_ratio: 0.0
    normalize: true
    normalization_method: standard
  independent_multi_vd:
    experiment_dir: blob/experiments/dev/lstm/independent_multi_vd
    log: logs/dev/train_independent_multi_vd_lstm.log
    report_log: "logs/dev/reports/generate_independent_multi_vd_report.log"  # Report generation log
    plot_log: "logs/dev/reports/generate_independent_multi_vd_plots.log"    # Plot generation log
    epochs: 10
    batch_size: 32
    sequence_length: 5
    num_vds: 1
    target_vd_index: 0
    model_type: lstm
    hidden_size: 32
    num_layers: 1
    dropout: 0.5
    learning_rate: 0.0005
    weight_decay: 0.01
    # Optimized training parameters from fixed configs
    early_stopping_patience: 8
    gradient_clip_value: 0.5
    use_scheduler: true
    scheduler_patience: 5
    # Dataset parameters
    prediction_length: 1
    train_ratio: 0.8
    val_ratio: 0.2
    test_ratio: 0.0
    normalize: true
    normalization_method: standard
training_xlstm:
  single_vd:
    experiment_dir: blob/experiments/dev/xlstm/single_vd
    log: logs/dev/train_single_vd_xlstm.log
    report_log: "logs/dev/reports/generate_single_vd_xlstm_report.log"  # Report generation log
    plot_log: "logs/dev/reports/generate_single_vd_xlstm_plots.log"    # Plot generation log
    epochs: 5
    batch_size: 16
    sequence_length: 5
    model_type: xlstm
    select_vd_id: VD-28-0740-000-001
    embedding_dim: 64
    num_blocks: 4
    slstm_at:
    - 1
    - 3
    context_length: 256
    dropout: 0.1
    backend: vanilla
    # Optimized training parameters from fixed configs
    learning_rate: 0.0005
    weight_decay: 0.01
    early_stopping_patience: 8
    gradient_clip_value: 0.5
    use_scheduler: true
    scheduler_patience: 5
    # Dataset parameters
    prediction_length: 1
    train_ratio: 0.8
    val_ratio: 0.2
    test_ratio: 0.0
    normalize: true
    normalization_method: standard
  multi_vd:
    experiment_dir: blob/experiments/dev/xlstm/multi_vd
    log: logs/dev/train_multi_vd_xlstm.log
    report_log: "logs/dev/reports/generate_multi_vd_xlstm_report.log"  # Report generation log
    plot_log: "logs/dev/reports/generate_multi_vd_xlstm_plots.log"    # Plot generation log
    epochs: 5
    batch_size: 16
    sequence_length: 5
    model_type: xlstm
    num_vds: 1
    embedding_dim: 64
    num_blocks: 4
    slstm_at:
    - 1
    - 3
    context_length: 256
    dropout: 0.1
    backend: vanilla
    # Optimized training parameters from fixed configs
    learning_rate: 0.0005
    weight_decay: 0.01
    early_stopping_patience: 8
    gradient_clip_value: 0.5
    use_scheduler: true
    scheduler_patience: 5
    # Dataset parameters
    prediction_length: 1
    train_ratio: 0.8
    val_ratio: 0.2
    test_ratio: 0.0
    normalize: true
    normalization_method: standard

# =============================================================================
# Social-xLSTM Training Configuration - Multi-VD Distributed Architecture
# =============================================================================
# Social-xLSTM 是 multi-VD 分散式架構，支援完整的空間社會聚合功能
# 注意：Social-xLSTM 本質上是 multi-VD 系統，如需 single VD 訓練請使用 training_xlstm
training_social_xlstm:
  multi_vd:
    experiment_dir: blob/experiments/dev/social_xlstm/multi_vd
    log: logs/dev/train_multi_vd_social_xlstm.log
    report_log: "logs/dev/reports/generate_multi_vd_social_xlstm_report.log"
    plot_log: "logs/dev/reports/generate_multi_vd_social_xlstm_plots.log"
    # Multi-VD configuration
    num_vds: 1
    # Model architecture parameters
    epochs: 10
    batch_size: 8
    sequence_length: 10
    prediction_length: 3
    hidden_size: 64
    num_blocks: 2
    embedding_dim: 64
    # xLSTM specific configuration
    slstm_at: [1]  # sLSTM layer positions (must be < num_blocks)
    # Social Pooling configuration (full functionality)
    enable_spatial_pooling: true
    spatial_radius: 2.0
    pool_type: mean
    learning_rate: 0.001
    # Optimized training parameters
    early_stopping_patience: 8
    gradient_clip_value: 0.5
    use_scheduler: true
    scheduler_patience: 5
    # Dataset parameters
    train_ratio: 0.8
    val_ratio: 0.2
    test_ratio: 0.0
    normalize: true
    normalization_method: standard
