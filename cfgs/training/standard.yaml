# Development Training Configuration
# Fast iteration with PyTorch Lightning standards
# Decoupled from model architecture and data parameters

trainer:
  max_epochs: 50

  # Let Lightning choose GPU/CPU automatically
  accelerator: "auto" 

  precision: "16-mixed"      
  devices: 1                
  
  enable_progress_bar: true
  log_every_n_steps: 10
  num_sanity_val_steps: 0
  check_val_every_n_epoch: 1

  callbacks:
    output_dir: change_cfgs_training_config_yaml_to_your_desired_path
    training_metrics:
      splits: [train, val]
      metrics: [mae, rmse, mape]
    model_checkpoint:
      monitor: "val_mae"
      mode: "min"
      save_top_k: 1
      filename: "epoch={epoch}-val_mae={val_mae:.4f}"

optimizer:
  # Adam optimizer configuration
  name: "Adam"
  lr: 0.001
  weight_decay: 0.0
  betas: [0.9, 0.999]
  eps: 1.0e-08

  
experiment:
  # Experiment tracking configuration
  name: "dev_multi_vd"
  save_dir: "logs"
  seed: 42                  # Reproducibility seed


